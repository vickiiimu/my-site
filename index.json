
[{"content":" Originally published on Medium .\n1. Opening: Why I Joined So Late # I first saw the Image Matching Challenge pop up on Kaggle sometime in early May. I bookmarked it out of curiosity — the problem sounded cool, and I’d been meaning to get into competitions for a while — but between MIT finals, end-of-semester deadlines, and packing up to move out, it got buried under everything else.\nIt wasn’t until a week before the deadline that I remembered it again. I was finally free, finally bored, and finally curious enough to open the starter notebook. One baseline submission later, I was hooked.\nThis was my first-ever Kaggle competition. I had no leaderboard expectations — just a vague goal to build something functional, learn as much as possible in seven days, and see how far I could push it.\n“It was a bit of a reckless decision. I had a week, a little familiarity with vision models, and zero competition experience. I wanted to see how far I could get.”\n2. Understanding the Challenge # The Image Matching Challenge (IMC) is an annual computer vision competition hosted by CVPR and Kaggle. At its core, the goal sounds simple: match local features between image pairs to estimate the relative pose of the cameras that captured them. In other words, given two images of the same scene — possibly from wildly different angles or lighting — can you determine how those cameras were positioned relative to each other?\nBut 2025’s edition took that basic setup and made it brutal.\nUnlike past IMC competitions, where the organizers provided pre-segmented scenes, this year you had to segment the scenes yourself. That meant reverse-engineering the scene structure from hundreds of loosely grouped image pairs before you could even begin estimating poses. It added an extra layer of uncertainty — you weren’t just solving the matching problem, you were solving the context problem, too.\nThese are two completely different stairwells — but my pipeline lumped them into the same scene more than once. Turns out ‘gray steps in a hallway’ isn’t a very unique visual signature. Source: Kaggle IMC 2025 What really separated IMC 2025 from class projects or standard CV pipelines was the real-world messiness of it all:\nImages weren’t neatly aligned or staged — they were taken from inconsistent angles, sometimes under drastically different lighting conditions, or even at different times of day Generalization mattered more than precision — your pipeline had to work across architecture, nature, objects, stairs, ET statues, and more And finally, compute was a bottleneck: every submission was a tradeoff between quality and runtime, with timeout risks looming 3. Days 1–2: Getting the Baseline Running # I kicked things off with a pipeline based on ALIKED + LightGlue — a lightweight but effective combo I’d seen mentioned in previous IMC discussions. ALIKED gave me reliable keypoints and descriptors, and LightGlue handled the matching with impressive robustness, especially in low-texture scenes. It was a solid starting point — but far from plug-and-play.\nThe real curveball in IMC 2025 was scene segmentation. Unlike in previous years, the dataset didn’t come pre-organized into scenes. That meant before I could even estimate poses, I had to figure out which image pairs belonged to the same physical environment.\nTo tackle that, I used DINOv2 as a global image descriptor. For each image, I extracted global embeddings using a pretrained DINOv2 backbone, then clustered the embeddings using KMeans to group images into scenes. It wasn’t perfect — and I definitely tuned it more by vibe than theory — but it was good enough to avoid total chaos.\nClustering results on the ET dataset using DINOv2 + KMeans. Two main clusters, with 3 outliers flagged. Source: author Still, some scenes like vineyard and stairs were incredibly brittle. These environments had so much repetition and structural symmetry that even small mistakes in clustering led to completely incorrect pose estimates. Multiple rows of grapevines or similar staircases taken at slightly different angles or lighting conditions could easily be grouped together, even when they came from different physical spaces. Those mistakes hit hard — the mAA metric punishes wrong matches aggressively, and once scene segmentation goes wrong, everything downstream follows.\nThree different vineyard scenes — visually similar enough to confuse DINOv2 and cluster them together, despite coming from completely different locations. These kinds of mistakes wrecked downstream pose estimates. Source: Kaggle IMC 2025 Because I had joined the competition so late, I leaned heavily on the Kaggle discussion boards to stay afloat. Reading through others’ pain points and quick wins helped me shortcut a lot of early confusion — especially around runtime optimization and evaluating failure cases like ET, stairs, and the infamous castle towers. I also saw a few people visualizing keypoint matches and inlier ratios, which helped me trust (or distrust) my pipeline more quickly.\nBy the end of Day 2, I had something functional. Not amazing — but stable, reproducible, and ready to start iterating on. The real experimentation was still ahead.\n4. Days 3–4: Idea Flood (and Reality Check) # By day three, I had a working baseline — which, naturally, meant I started throwing every half-baked idea I had at the wall to see what might stick. At one point, my notebook was a graveyard of TODOs: local SfM pipelines, GNN-based outlier rejection, even patch-wise attention reranking. Some of these I partially implemented. Others I just mocked up enough to convince myself I didn’t have the time.\nBut a few things did make it into the actual pipeline:\nCluster refinement via match graph connectivity: I built a scene graph where each node was an image, and edges were weighted by LightGlue match confidence and number of inliers. Then I ran connected component analysis to recover tightly linked subgraphs — effectively letting the data self-organize the scene clusters. This was especially useful in ambiguous environments like vineyards and stairs, where DINOv2 global embeddings alone tended to over-group visually similar but spatially unrelated scenes.\nTargeted COLMAP reruns with pre-filtered matches: I selectively reran COLMAP on scenes with low estimated mAA, injecting LightGlue matches with custom outlier filtering. This worked well for scenes with harsh lighting changes or wide baselines — particularly the ET statue and several of the indoor staircases, where vanilla matching often produced degenerate poses.\nPose-aware pair filtering: For a brief window, I experimented with using pairwise match consistency — essentially a simplified geometric verification — to discard scene pairs that weren’t consistent with the rest of their cluster. It was light-touch, but reduced false positives in scenes with high symmetry.\nA few other ideas didn’t make the cut, but might in a future comp:\nLearning a per-scene descriptor weighting strategy (foreground vs. background emphasis) Filtering match graphs using spectral clustering on pose residuals Using CLIP embeddings as a second-pass sanity check for scene consistency Ultimately, I realized I wasn’t going to outbuild the top teams — not in a week — but I could be thoughtful about what not to waste time on. That became my mini-strategy: solve what’s fragile, trust what’s robust, and don’t touch anything on Friday night.\n5. Days 5–6: Patch, Tune, Pray # By this point, it wasn’t about adding features — it was about pipeline triage.\nDays 5 and 6 were a blur of last-minute patches, scene-level debugging, and running the full dataset through my pipeline without it catching fire. I knew I was running out of time, and the priority shifted from “can I improve this?” to “can I trust this?”\nThe real MVPs were all the print statements. Source: author I focused on final structure: stitching together my best scene clustering logic (DINOv2 + match-graph analysis), a couple custom COLMAP reruns for the weird scenes (looking at you, ET), and a filtering layer for pruning obviously bad pairs. Stairs remained one of the hardest — visually repetitive, sometimes indoor, sometimes outdoor, and prone to degeneracy in pose estimation. My only move there was to hand-check a few sample scenes and hope the outlier filter was doing its job.\nMeanwhile, compute became a bottleneck. I was trying to test full runs locally, monitor GPU usage on Kaggle notebooks, and avoid timeout errors — all while juggling submission caps. At one point I split inference across multiple notebooks just to stay under the limit. It felt duct-taped together, but at least it was running.\nMy notebook was a Frankenstein of late-night hacks, quick sanity checks, and a few very strategic print()s. Honestly, it had no right to work as well as it did.\n6. Day 7: Submitting + The Final Rank # Submission day felt like holding my breath for 12 hours straight.\nI queued up my final pipeline the night before the deadline — patched, filtered, clustered, and barely tested end-to-end. It wasn’t clean, and it definitely wasn’t elegant, but it ran. And at that point, that was all I needed.\nWhen the results came in, I landed at Rank 260 out of 943.\nFinal leaderboard rank: 260 out of 943 — not a medal, but a 501-place climb in seven days, and my first ever Kaggle comp. I’ll take it. Source: author No medal, no spotlight — but for a first competition done in seven days, while figuring out scene segmentation, pose estimation, LightGlue, COLMAP, and Kaggle notebooks on the fly\u0026hellip; I was proud of that number.\nIt wasn’t about beating anyone else. It was about proving to myself that I could build something from scratch, debug under pressure, and ship a real submission. And I did.\nI didn’t expect a medal — but seeing my name halfway up the leaderboard felt like proof of concept: I could do this.\nWhat one week of late nights, weird bugs, and unexpected breakthroughs looked like. The timeline of how I (barely) held it all together. Source: author 7. What I Learned in 7 Days # This competition condensed months of learning into a single week. I came in with some computer vision experience — but matching, clustering, and pose estimation under pressure taught me things no lecture or tutorial ever could.\nReproducibility is non-negotiable # When you’re iterating fast, the ability to rerun the exact same pipeline is everything. I lost hours to silent mismatches between intermediate files, cache conflicts, and half-tested logic blocks. By day 3, I was versioning configs and scene outputs like my life depended on it — because it kind of did.\nVisual matching is way harder than it looks # On paper, it’s just keypoints and descriptors. In reality, it’s everything from occlusions to lighting variation, repeating structures, degenerate geometry, and weird edge cases that no tutorial prepares you for. Robustness mattered more than elegance.\nThe leaderboard can help — or totally distract # Seeing your name rise is addictive, but chasing short-term gains made me overfit to specific scenes more than once. Some of my lowest mAA scores came from over-optimized fixes that broke generalization. In the end, the most helpful metric was just consistency across scene types.\nCompetitions make you a faster, better debugger # I had no choice but to figure things out quickly: mismatched pose outputs, LightGlue breaking silently, scenes clustering wrong, COLMAP throwing obscure errors\u0026hellip; All of it sharpened my instincts. I now write defensive code by default — with checks, asserts, and visualizations built-in.\nMore than anything, I learned that these comps are a perfect training ground for research-style thinking: you pick your assumptions, fail fast, validate hard, and trust nothing until it works on the leaderboard.\n8. What I’d Do Differently # Looking back, there’s a lot I’m proud of — but also a lot I’d change if I had more time (or a time machine).\nStart earlier. Seriously. # One week was barely enough to get a stable pipeline running, let alone explore the frontier of matching models. I spent more time fixing data issues than testing ideas — and missed out on the deep modeling work that could’ve made a real difference.\nGet more aggressive with bad pairs # Some of my lowest-scoring scenes were due to noisy matches that should’ve been filtered out earlier — either by geometric inconsistency, low keypoint density, or scene confusion. A stronger outlier rejection module (maybe even learned) would’ve saved me a lot of leaderboard pain.\nExplore beyond the baseline # I stuck with ALIKED + LightGlue for speed and familiarity, but next time I’d love to seriously try:\nLoFTR for its dense, global attention-driven matching GNN-based match filtering to reason about match confidence beyond pairwise similarity And maybe even pose refinement modules post-COLMAP, built to adapt to challenging camera configurations If I’d had two more weeks, I would’ve focused on generalization: building a model-aware filtering stage and validating it across clusters, rather than hand-tuning scenes one-by-one. That’s the kind of robustness that separates mid-tier submissions from gold medals.\n9. What’s Next # I’m not entirely sure what my next competition will be — but I know this sprint wasn’t a one-off.\nI’m currently deciding between a few very different challenges:\nCMI — Detect Behavior with Sensor Data, to sharpen temporal modeling and classification robustness NeurIPS — Open Polymer Prediction, for a deep dive into scientific ML and graph-based modeling Or DRW — Crypto Market Prediction, a fast-paced signal extraction challenge that forces you to think like a quant Whichever I choose, the goal is clear: medal in my second comp, and keep moving toward true mastery.\nThis sprint was a turning point. It reminded me why I do this — not just to rank, but to build, to learn fast, and to get pushed beyond the comfort zone of lectures and controlled assignments. I learned more in one week of hacking together a real pipeline than I have in some entire semesters. I learned what to prioritize, what to throw away, how to debug under pressure, and how to focus only on what matters when time is short.\nKaggle has become part of my larger AI journey — a place where I can stay sharp, stay uncomfortable, and build the kind of intuition that only comes from doing. It’s not always pretty. But it’s real.\nAbout Me # I’m a sophomore at MIT studying physics and artificial intelligence. This post is part of my Projects in the Wild series — where I document personal experiments, Kaggle challenges, physics-ML crossovers, and anything else that pushes me outside the classroom.\nMy goal is to learn by building: fast, messy, sometimes late, but always real. Whether it’s reimplementing papers or racing the Kaggle deadline, I use these projects to sharpen my instincts and stay uncomfortable — the kind of learning that doesn’t come from lectures alone.\nIf you enjoyed this post, feel free to follow me on GitHub , Medium , Twitter , or reach out . I love talking about AI, open-source, and anything that blends math, vision, or modeling under pressure.\n","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/posts/kaggle_imc/","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eOriginally published on \u003ca\n  href=\"https://medium.com/@vicki.y.mu/beginning-on-the-leaderboard-my-first-kaggle-challenge-in-7-days-0749ceebc825\"\n    target=\"_blank\"\n  \u003eMedium\u003c/a\u003e\n.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\n\u003ch2 class=\"relative group\"\u003e1. Opening: Why I Joined So Late \n    \u003cdiv id=\"1-opening-why-i-joined-so-late\" class=\"anchor\"\u003e\u003c/div\u003e\n    \n    \u003cspan\n        class=\"absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100\"\u003e\n        \u003ca class=\"group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline\" href=\"#1-opening-why-i-joined-so-late\" aria-label=\"Anchor\"\u003e#\u003c/a\u003e\n    \u003c/span\u003e        \n    \n\u003c/h2\u003e\n\n\u003cfigure\u003e\n        \u003cimg\n          class=\"my-0 rounded-md\"\n          srcset=\"\n          /my-site/posts/kaggle_imc/imc2025_hu_fdb36397f1ae3de1.png 330w,\n          /my-site/posts/kaggle_imc/imc2025_hu_7cf76a832433ed9b.png 660w,\n          /my-site/posts/kaggle_imc/imc2025_hu_935bf3ea4bf60ec0.png 1024w,\n          /my-site/posts/kaggle_imc/imc2025_hu_2d4a19e22af0a180.png 2x\"\n          src=\"/my-site/posts/kaggle_imc/imc2025_hu_7cf76a832433ed9b.png\"\n          data-zoom-src=\"/my-site/posts/kaggle_imc/imc2025_hu_2d4a19e22af0a180.png\"\n          alt=\"\"\n        /\u003e\n  \n  \n  \u003c/figure\u003e\n\u003cp\u003eI first saw the Image Matching Challenge pop up on Kaggle sometime in early May. I bookmarked it out of curiosity — the problem sounded cool, and I’d been meaning to get into competitions for a while — but between MIT finals, end-of-semester deadlines, and packing up to move out, it got buried under everything else.\u003c/p\u003e","title":"Beginning on the Leaderboard: My First Kaggle Challenge in 7 Days","type":"posts"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/tags/computer-vision/","section":"Tags","summary":"","title":"Computer Vision","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/tags/deep-learning/","section":"Tags","summary":"","title":"Deep Learning","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/tags/kaggle/","section":"Tags","summary":"","title":"Kaggle","type":"tags"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/categories/projects-in-the-wild/","section":"Categories","summary":"","title":"Projects in the Wild","type":"categories"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/series/projects-in-the-wild/","section":"Series","summary":"","title":"Projects in the Wild","type":"series"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"🛠️ Currently\n• Building a LoRA finetuning heuristic\n• Reading: Flow Matching for Generative Modeling\n","date":"30 June 2025","externalUrl":null,"permalink":"/my-site/","section":"Vicki Mu","summary":"\u003cp\u003e🛠️ Currently\u003cbr\u003e\n• Building a LoRA finetuning heuristic\u003cbr\u003e\n• Reading: \u003cem\u003eFlow Matching for Generative Modeling\u003c/em\u003e\u003c/p\u003e","title":"Vicki Mu","type":"page"},{"content":" Originally published on Medium .\nConvolutional neural networks have been the driving force behind almost every major breakthrough in computer vision — but what if they’ve been holding us back all along?\nIn 2020, a team of researchers at Google asked the bold question: Can we throw out convolutions entirely and still build world-class image models?\nTheir answer — the Vision Transformer (ViT)— sparked a new era in deep learning.\nFigure 1: Vision Transformer (ViT) architecture. Source: Dosovitskiy et al., 2020 (arXiv:2010.11929 ) I\u0026rsquo;m an undergraduate student at MIT with an interest in computer vision and generative models, and I recently implemented a Vision Transformer from scratch to better understand its architecture. This post is a distilled guide to that process, blending theory, visuals and code.\nWe\u0026rsquo;ll walk through how ViTs turn images into sequences, how attention works in this context, and how it compares to the CNNs you\u0026rsquo;re used to. By the end, you’ll have a working ViT in PyTorch and a much deeper understanding of how it all fits together.\n1. Background and Intuition # 1.1 From Recurrent Models to the Rise of Transformers (in NLP) # Before 2017, NLP was ruled by RNNs and LSTMs: models that powered everything from machine translation to language modeling. But despite their early success, they had fundamental limitations. Because they processed sequences one token at a time, training couldn’t be parallelized. And as sequences got longer, they struggled to retain information from earlier tokens. These bottlenecks made it difficult to scale up, especially for tasks that required a deep, global understanding of language.\nIn 2017, researchers at Google made a breakthrough in their paper Attention Is All You Need . It proposed a new architecture — the Transformer — built around a simple but powerful idea: self-attention. Instead of processing tokens one at a time, self-attention allows each token to directly consider every other token in the sequence.\nPut simply, each word learns to ask questions (queries), decide what to listen to (keys), and gather relevant information (values).\nThis mechanism eliminated the need for recurrence and fixed step order, sidestepping the main weaknesses of RNNs.\nFigure 2: RNNs handle inputs sequentially, while Transformers attend to all tokens in parallel. Line thickness represents attention strength. Source: author. Within just two years, Transformer architecture completely took over NLP.\nIt proved more efficient, easier to scale, and better at modeling long-range dependencies than any of its predecessors. Transformers quickly became the backbone of major breakthrough models: BERT (for bi-directional context), GPT (for generative, causal language modeling), and T5 (for sequence to sequence tasks).\nRNNs were replaced by attention in NLP — but what about computer vision?\nAt the time, CNNs dominated the field, but they came with their own set of limitations. Convolutions are inherently local, making it difficult for CNNs to capture long-range dependencies. They also rely heavily on spatial prior and careful feature engineering.\nSo the natural next question emerged: if attention could replace recurrence… could it replace convolution too?\n1.2 Can Attention Replace Convolution? The Shift to Vision # In 2020, Dosovitskiy et al. introduced the Vision Transformer (ViT) in their paper An Image Is Worth 16×16 Words . They proposed a bold idea: what if we treated an image like a sentence?\nInstead of relying on convolutional filters, they divided images into patches and fed them into a standard transformer. While early ViTs needed massive datasets to compete with CNNs, the approach proved that attention-based models could work for vision—not just language.\nSince its release, the Vision Transformer has sparked a wave of improvements:\nDeiT introduced smarter training strategies to reduce ViT’s reliance on huge datasets Swin Transformer added hierarchical structure to better handle local spatial patterns DINO and DINOv2 showed that ViTs could learn rich visual representations without any labels at all—unlocking powerful self-supervised features for downstream tasks. What began as a bold experiment has now become a core building block in modern computer vision.\n2. How Vision Transformers Work # 2.1 Patch Embedding # Transformers were originally designed to process sequences, like sentences made out of word tokens. But images are 2D grids of pixels, not 1D sequences. So how do we feed an image into a transformer?\nThe Vision Transformer solves this by dividing the image into non-overlapping square patches (e.g. 16x16 pixels). Each patch is then flattened into a 1D vector and linearly projected into a fixed-size embedding—just like token embeddings in NLP.\nFor example:\nA 224x224 image with 16x16 patches produces (224/16)² = 196 patches. Each patch is of shape 3x16x16 (RGB). Each flattened patch becomes a 768-dim vector (common for ViT-Base). Instead of a sentence of words, ViT sees a sequence of image patch embeddings.\nFigure 3: Patch embedding. Source: Dosovitskiy et al., 2020 Analogy: Just like a tokenizer turns a sentence into a sequence of word embeddings, the ViT turns an image into a sequence of patch embeddings.\n2.2 Class Token and Positional Embeddings # Transformers need two extra ingredients to work properly with image sequences:\na [CLS] token to aggregate global information, and positional embeddings to encode spatial structure. In ViT, a special learnable token is prepended to the input sequence. During self-attention, this token attends to every patch—and becomes the representation used for final classification.\nTransformers are permutation-invariant—they don’t inherently understand order. To give the model spatial awareness, we add a unique positional embedding to each token in the sequence.\nBoth the [CLS] token and positional embeddings are learned parameters, updated during training.\n2.3 Multi-Head Self-Attention (MHSA) # At the heart of the Vision Transformer is the multi-head self-attention mechanism—the part that allows the model to understand how image patches relate to each other, regardless of spatial distance.\nInstead of using one attention function, MHSA splits the input into multiple “heads”. Each head learns to focus on different aspects of the input—some might focus on edges, others on texture, others on spatial layout. Their outputs are then concatenated and projected back into the original embedding space.\nHow it works, step by step:\nThe input sequence of tokens (shape [B, N, D]) is linearly projected into: Queries Q, Keys K, and Values V. Each attention head computes: Equation 1: Scaled Dot-Product Attention Multiple heads run in parallel, and their outputs are concatenated and linearly projected back. Why “multi-head”?\nEach head attends to different parts of the sequence. This allows the model to understand complex relationships in parallel—not just spatial proximity, but also semantic structure.\nFigure 4: The left shows how attention scores are computed using queries (Q), keys (K), and values (V). The right illustrates how multiple attention \u0026lsquo;heads\u0026rsquo; capture diverse representations in parallel. Source: Vaswani et al. (arXiv:1706.03762 ) 2.4 Transformer Encoder # Once we have self-attention, we wrap it inside a larger unit: the Transformer block. This block is the fundamental building unit of ViTs (and NLP Transformers too). It combines:\nLayerNorm → Multi-Head Attention → Residual Connection LayerNorm → MLP (Feedforward Network) → Residual Connection Each block enables the model to attend globally and transform features across layers while maintaining stability with normalization and residuals.\nWhat’s inside a ViT Transformer block:\nLayerNorm before attention (called pre-norm). Multi-head self-attention applied to the normalized input. A residual connection adds the attention output back. Another LayerNorm, followed by a small MLP. Another residual connection adds the MLP output. Figure 5: ViT encoder block with attention, MLP, and residuals. Source: author This structure repeats across all transformer layers (e.g., 12 layers in ViT-Base).\n2.5 Classification Head # After processing the input through multiple Transformer blocks, the model needs a way to produce a final prediction. In Vision Transformers, this is handled by the classification head.\nDuring the embedding step, we added a special [CLS] token at the beginning of the sequence. Just like in BERT, this token is intended to aggregate information from all the image patches through self-attention. After passing through all Transformer layers, the final embedding of the [CLS] token is used as a summary representation of the entire image.\n3. Implementation Walkthrough # All core modules—patch embedding, MHSA, encoder blocks—are implemented from scratch. No timm shortcuts.\n3.1 Patch Embedding # To convert image patches into a sequence of embeddings, we use a clever trick. Instead of writing a manual for-loop to extract and flatten patches, we can use a Conv2d layer with:\nkernel_size = patch_size stride = patch_size This extracts non-overlapping patches and applies a learned linear projection—all in a single operation. It’s clean, efficient, and easy to backpropagate through.\nclass PatchEmbed(nn.Module): def __init__(self, img_size = 224, patch_size = 16, in_chans = 3, embed_dim = 768): super().__init__() self.img_size = img_size self.patch_size = patch_size self.num_patches = (img_size // patch_size) ** 2 self.proj = nn.Conv2d( in_chans, embed_dim, kernel_size = patch_size, stride = patch_size) def forward(self, x): # x shape: [B, 3, 224, 224] x = self.proj(x) # [B, emdbed_dim, H/patch, W/patch] x = x.flatten(2) # [B, emdbed_dim, num_patches] x = x.transpose(1, 2) # [B, num_patches, embed_dim] return x 3.2 Class Token and Positional Embeddings # Here we define a ViTEmbed module that:\nPrepends a learnable [CLS] token to the sequence Adds a learnable positional embedding to each token (including [CLS]) This produces a sequence shaped [B, num_patches + 1, embed_dim] — ready for the transformer encoder.\nclass ViTEmbed(nn.Module): def __init__(self, num_patches, embed_dim): super().__init__() # Learnable [CLS] token (1 per model) self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) # [1, 1, D] # Learnable positional embeddings (1 per token, including CLS) self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim)) # [1, N+1, D] def forward(self, x): batch_size = x.shape[0] # Expand [CLS] token to match batch size cls_tokens = self.cls_token.expand(batch_size, -1, -1) # [B, 1, D] # Prepend CLS token to patch embeddings x = torch.cat((cls_tokens, x), dim=1) # [B, N+1, D] # Add positional embeddings x = x + self.pos_embed # [B, N+1, D] return x 3.3 Multi-Head Self-Attention # Let’s implement one of the most important parts of the Vision Transformer: multi-head self-attention.\nEach input token is linearly projected into a query (Q), key (K), and value (V) vector. Attention is computed in parallel across multiple heads, then concatenated and projected back to the original embedding dimension.\nclass MyMultiheadAttention(nn.Module): def __init__(self, embed_dim, num_heads): super().__init__() assert embed_dim % num_heads == 0, \u0026#34;embed_dim must be divisible by num_heads\u0026#34; self.embed_dim = embed_dim self.num_heads = num_heads self.head_dim = embed_dim // num_heads # Learnable projections for Q, K, V self.q_proj = nn.Linear(embed_dim, embed_dim) self.k_proj = nn.Linear(embed_dim, embed_dim) self.v_proj = nn.Linear(embed_dim, embed_dim) # Final output projection self.out_proj = nn.Linear(embed_dim, embed_dim) def forward(self, x): B, T, C = x.shape # [batch, seq_len, embed_dim] # Project input into Q, K, V Q = self.q_proj(x) K = self.k_proj(x) V = self.v_proj(x) # Reshape into heads: [B, num_heads, T, head_dim] def split_heads(tensor): return tensor.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) Q = split_heads(Q) K = split_heads(K) V = split_heads(V) # Scaled dot-product attention scores = torch.matmul(Q, K.transpose(-2, -1)) # [B, heads, T, T] scores /= self.head_dim ** 0.5 attn = torch.softmax(scores, dim=-1) # Apply attention to values out = torch.matmul(attn, V) # [B, heads, T, head_dim] # Recombine heads out = out.transpose(1, 2).contiguous().view(B, T, C) # Final linear projection return self.out_proj(out) 3.4 Transformer Encoder # We now wrap everything together in a Transformer block — a modular unit that stacks self-attention and MLP layers with residual connections. This design lets the model reason globally (through self-attention) and then transform those representations (through the MLP), all while preserving stability via skip connections.\nIn this implementation:\nWe use our own MyMultiheadAttention class from earlier to demystify how attention works under the hood. In practice, you can use PyTorch’s built-in nn.MultiheadAttention for convenience and efficiency. We apply LayerNorm before both the attention and MLP layers (a “pre-norm” design). The mlp_ratio controls the width of the MLP’s hidden layer (usually 3–4× wider than the embedding dimension). Let’s build the full Transformer block:\nclass TransformerBlock(nn.Module): def __init__(self, embed_dim, num_heads, mlp_ratio=4.0): super().__init__() self.norm1 = nn.LayerNorm(embed_dim) self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True) self.norm2 = nn.LayerNorm(embed_dim) self.mlp = nn.Sequential( nn.Linear(embed_dim, int(embed_dim * mlp_ratio)), nn.GELU(), nn.Linear(int(embed_dim * mlp_ratio), embed_dim) ) def forward(self, x): # Self-attention with residual connection x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0] # MLP with residual connection x = x + self.mlp(self.norm2(x)) return x 3.5 Putting It All Together # Now that we’ve built all the key components of a Vision Transformer — patch embedding, positional encoding, multi-head self-attention, Transformer blocks, and the [CLS] token — it’s time to assemble everything into a full model.\nIn the code below:\nWe use our PatchEmbed, ViTEmbed, and TransformerBlock classes from earlier. The [CLS] token is passed through all transformer layers and then normalized. We add a classification head: a single nn.Linear layer that maps the [CLS] token embedding to class logits. This architecture mirrors the original ViT-Base (12 layers, 12 heads, 768-dim embeddings), but it’s easy to scale. class SimpleViT(nn.Module): def __init__( self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12, num_classes=1000 ): super().__init__() self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim) num_patches = (img_size // patch_size) ** 2 self.vit_embed = ViTEmbed(num_patches, embed_dim) # Stack transformer blocks self.blocks = nn.Sequential(*[ TransformerBlock(embed_dim, num_heads) for _ in range(depth) ]) # Final normalization before classification self.norm = nn.LayerNorm(embed_dim) # Linear classification head (using the CLS token) self.head = nn.Linear(embed_dim, num_classes) def forward(self, x): # [batch_size, channels, height, width] x = self.patch_embed(x) # -\u0026gt; [B, N, D] x = self.vit_embed(x) # add CLS token + pos embed x = self.blocks(x) # transformer layers x = self.norm(x) # normalize CLS token return self.head(x[:, 0]) # classification using CLS token 4. Training the ViT # 4.1 Dataset: CIFAR-10 # We trained our Vision Transformer (ViT) on CIFAR-10, a well-known benchmark dataset with 60,000 images across 10 classes (e.g., airplanes, cats, ships). Each image is only 32×32 pixels, which makes CIFAR-10:\nLightweight and quick to train on Challenging enough to evaluate small models Easy to visualize, which helps interpret what the model is learning 4.2 Model Setup: Adapting ViT for CIFAR-10 # ViTs were originally designed for large-scale datasets like ImageNet, so we made several adjustments to make training feasible on CIFAR-10 with limited compute:\nInput size: Resized to 32×32 to match CIFAR-10 Patch size: 4×4 → yields 64 tokens per image Embedding dimension: 192 (smaller than ImageNet-scale ViTs) Depth: 6 transformer blocks Heads: 3 attention heads (192 ÷ 3 = 64 dim per head) Positional embeddings: Recomputed for 64+1 tokens Batch size: 80 — balances speed and memory on Colab # Refactored SimpleViT config for CIFAR-10 model = SimpleViT( img_size=32, # CIFAR-10 images are 32x32 patch_size=4, # 4x4 patches → 64 tokens in_chans=3, embed_dim=192, # Smaller embedding size depth=6, # Fewer transformer blocks num_heads=3, # Divides evenly into 192 num_classes=10 # For CIFAR-10 ).to(device) 4.3 Training Setup # The model was trained using:\nOptimizer: Adam (lr = 3e-4) Loss: CrossEntropyLoss Hardware: Google Colab T4 GPU Training was efficient — about 30 seconds per epoch, thanks to:\nFewer transformer blocks and tokens Larger batch size (80) Explicit use of CUDA (to(device)) 4.3 Results # We trained our Vision Transformer for 30 epochs, totaling ~15 minutes on a GPU. By the end of training, the model achieved approximately 60% accuracy on the CIFAR-10 test set — a solid baseline given the model’s simplicity and the relatively small dataset size.\nAs shown in the training plots below:\nTraining loss steadily decreased, indicating that the model was effectively minimizing prediction error on the training set. Test accuracy improved rapidly within the first 10 epochs, plateauing around 60% thereafter. This suggests the model learned quickly but struggled to generalize further without additional techniques like data augmentation or regularization. Figure 6: Training loss and test accuracy over 30 epochs. Source: author Here are a few example outputs from the model. While it correctly identified many samples (like cats and frogs), it struggled with visually similar classes (e.g., misclassifying a ship as an airplane).\nFigure 7: Example predictions on CIFAR-10 images. Source: author The bar chart below shows how well the model performed across all 10 classes. Notably:\nThe model performed best on ship, automobile, and frog classes — likely due to more distinctive visual features. Performance lagged on cat and bird, which may be harder to distinguish due to higher intra-class variation and similar textures or shapes shared with other animals. Figure 8: Accuracy by class. Source: author 5. Limitations and Extensions # Despite their success, Vision Transformers (ViTs) come with trade-offs. Here’s a summary of what to keep in mind:\n5.1 Limitations # Data-Hungry by Design\nViTs lack the strong inductive biases of CNNs (like locality and translation invariance), which means they typically require large datasets to perform well.\n→ This is why the original ViT was pretrained on massive private datasets.\nQuadratic Time Complexity\nThe self-attention mechanism scales with the square of the number of input tokens — making ViTs computationally expensive for high-resolution images. For an image split into N patches, attention scales as O(N²).\n5.2 Extensions and Improvements # Researchers have developed several workarounds and improvements to address these issues:\nDeiT (Data-efficient Image Transformer)\nA version of ViT trained without large private datasets, using knowledge distillation from a CNN teacher to improve performance on smaller datasets like ImageNet.\nPretrained Backbones + Fine-Tuning\nInstead of training ViTs from scratch, most modern pipelines use pretrained ViTs and then fine-tune them on downstream tasks with fewer samples.\nSwin Transformer\nIntroduces a hierarchical structure similar to CNNs by using local window-based attention that shifts across layers — making it efficient and scalable for high-resolution inputs.\nFine-tuning on Small Datasets\nTechniques like freezing early layers, adding task-specific heads, or leveraging self-supervised pretraining (e.g., DINO, MAE) can help ViTs adapt well to limited data.\nIn short, while ViTs opened the door to attention-based vision modeling, their full potential is best realized when paired with large-scale pretraining, architectural tweaks, or smart training tricks.\n6. GitHub + Colab # View the Github # Includes a clean folder structure with:\nvit_cifar10.ipynb notebook images/ folder for visualizations requirements.txt for easy installation Open in Colab # Readers can fork and run the notebook directly in the browser.\nInstallation # git clone https://github.com/vickiiimu/vit-cifar10-tutorial.git cd vit-cifar10-tutorial pip install -r requirements.txt 7. Conclusion # Congratulations — you’ve just built a Vision Transformer from scratch!\nAlong the way, we covered the intuitions behind attention, walked through ViT’s architecture block-by-block, and reimplemented core components like patch embedding, positional encoding, and multi-head self-attention. If you followed the walkthrough, you now have a functioning ViT model you fully understand.\nWhether you’re here to learn the internals, prototype your own vision ideas, or just scratch the itch of curiosity — this is your sandbox.\nFeedback welcome # If you spot bugs, have suggestions, or build something cool on top of this, feel free to open an issue or pull request on GitHub.\nFurther reading # An Image is Worth 16x16 Words (ViT paper) Attention is All You Need (Transformer paper) DeiT: Data-efficient training of ViTs Thanks for reading! 👋\nSee you in the next post.\nAbout Me # I’m a sophomore at MIT studying physics and artificial intelligence. This post is part of a series — Recreated from Scratch — where I reimplement foundational AI papers to learn them inside and out. I’ll be posting new walkthroughs every few weeks, diving into a different model or paper each time.\nIf you enjoyed this walkthrough, feel free to follow me on GitHub , follow me on Medium , or even reach out . I love chatting about research, open-source projects, and all things deep learning.\n","date":"26 June 2025","externalUrl":null,"permalink":"/my-site/posts/vit/","section":"Posts","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eOriginally published on \u003ca\n  href=\"https://pub.towardsai.net/from-pixels-to-predictions-building-a-transformer-for-images-fea5a4f64816\"\n    target=\"_blank\"\n  \u003eMedium\u003c/a\u003e\n.\u003c/em\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eConvolutional neural networks have been the driving force behind almost every major breakthrough in computer vision — \u003cstrong\u003ebut what if they’ve been holding us back all along?\u003c/strong\u003e\u003c/p\u003e","title":"From Pixels to Predictions: Building a Transformer for Images","type":"posts"},{"content":"","date":"26 June 2025","externalUrl":null,"permalink":"/my-site/categories/recreated-from-scratch/","section":"Categories","summary":"","title":"Recreated From Scratch","type":"categories"},{"content":"","date":"26 June 2025","externalUrl":null,"permalink":"/my-site/series/recreated-from-scratch/","section":"Series","summary":"","title":"Recreated From Scratch","type":"series"},{"content":"","date":"26 June 2025","externalUrl":null,"permalink":"/my-site/tags/transformers/","section":"Tags","summary":"","title":"Transformers","type":"tags"},{"content":"I\u0026rsquo;m a sophomore at MIT studying AI and physics. This site is my public lab notebook—a space where I break down ideas, build things, and try to make sense of what I\u0026rsquo;m learning.\nI write about: # Recreated from Scratch: Reimplementations of foundational AI papers (transformers, diffusion, etc.) with annotated code and commentary.\nProjects in the Wild: Personal experiments, Kaggle challenges, physics-ML crossovers, and anything else I can’t stop thinking about.\nWhether you\u0026rsquo;re here to explore math, follow a tutorial, or just browse something unexpected, thanks for visiting.\nOpen to collaborating—especially if you\u0026rsquo;re working on something cool in AI or research you\u0026rsquo;d like to riff on.\n📮 Reach me: vymu@mit.edu ","externalUrl":null,"permalink":"/my-site/about/","section":"Vicki Mu","summary":"\u003cp\u003eI\u0026rsquo;m a sophomore at MIT studying AI and physics. This site is my public lab notebook—a space where I break down ideas, build things, and try to make sense of what I\u0026rsquo;m learning.\u003c/p\u003e","title":"About Me","type":"page"},{"content":"","externalUrl":null,"permalink":"/my-site/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"}]