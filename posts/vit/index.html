<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"><head>
  <meta charset="utf-8" />
  
  <meta http-equiv="content-language" content="en" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title>From Pixels to Predictions: Building a Transformer for Images &middot; For Later Analysis</title>
  <meta name="title" content="From Pixels to Predictions: Building a Transformer for Images &middot; For Later Analysis" />
  
  <meta name="description" content="A walkthrough of how Vision Transformers work, how to implement one, and why it marked a major shift in computer vision." />
  <meta name="keywords" content="deep learning, transformers, computer vision, " />
  
  
  <link rel="canonical" href="https://vickiiimu.github.io/my-site/posts/vit/" />
  
  
  
  
  
  
  
  
  
  
  <link type="text/css" rel="stylesheet" href="/my-site/css/main.bundle.min.cad6938d15c66b377e690b1062db58828f0e8c03daaadb417e12924222a1743960fee71c177591c6b2cfd200358db3af695743955beb457013f1e92bdb680cdd.css"
    integrity="sha512-ytaTjRXGazd&#43;aQsQYttYgo8OjAPaqttBfhKSQiKhdDlg/uccF3WRxrLP0gA1jbOvaVdDlVvrRXAT8ekr22gM3Q==" />
  
  
  <script type="text/javascript" src="/my-site/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <script defer type="text/javascript" id="script-bundle" src="/my-site/js/main.bundle.min.dc15078f1fe553ce354f85ec1bfce7ded8b4060d4124a49b19e31dd64345b8d8f03edbbb1b681748650181f9d81da9728ca10c2b9cb2ae7bf8983d4b669ba876.js"
    integrity="sha512-3BUHjx/lU841T4XsG/zn3ti0Bg1BJKSbGeMd1kNFuNjwPtu7G2gXSGUBgfnYHalyjKEMK5yyrnv4mD1LZpuodg==" data-copy="Copy" data-copied="Copied"></script>
  
  
  
  <script src="/my-site/lib/zoom/zoom.min.f592a181a15d2a5b042daa7f746c3721acf9063f8b6acd175d989129865a37d400ae0e85b640f9ad42cd98d1f8ad30931718cf8811abdcc5fcb264400d1a2b0c.js" integrity="sha512-9ZKhgaFdKlsELap/dGw3Iaz5Bj&#43;Las0XXZiRKYZaN9QArg6FtkD5rULNmNH4rTCTFxjPiBGr3MX8smRADRorDA=="></script>
  
  
  
  <link rel="apple-touch-icon" sizes="180x180" href="/my-site/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/my-site/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/my-site/favicon-16x16.png" />
  <link rel="manifest" href="/my-site/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="https://vickiiimu.github.io/my-site/posts/vit/">
  <meta property="og:site_name" content="For Later Analysis">
  <meta property="og:title" content="From Pixels to Predictions: Building a Transformer for Images">
  <meta property="og:description" content="A walkthrough of how Vision Transformers work, how to implement one, and why it marked a major shift in computer vision.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-06-26T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-06-26T00:00:00+00:00">
    <meta property="article:tag" content="Deep Learning">
    <meta property="article:tag" content="Transformers">
    <meta property="article:tag" content="Computer Vision">
    <meta property="og:image" content="https://vickiiimu.github.io/my-site/posts/vit/featured.png">

  
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://vickiiimu.github.io/my-site/posts/vit/featured.png">
  <meta name="twitter:title" content="From Pixels to Predictions: Building a Transformer for Images">
  <meta name="twitter:description" content="A walkthrough of how Vision Transformers work, how to implement one, and why it marked a major shift in computer vision.">

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "From Pixels to Predictions: Building a Transformer for Images",
    "headline": "From Pixels to Predictions: Building a Transformer for Images",
    "description": "A walkthrough of how Vision Transformers work, how to implement one, and why it marked a major shift in computer vision.",
    "abstract": "\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eOriginally published on \u003ca\n  href=\u0022https:\/\/pub.towardsai.net\/from-pixels-to-predictions-building-a-transformer-for-images-fea5a4f64816\u0022\n    target=\u0022_blank\u0022\n  \u003eMedium\u003c\/a\u003e\n.\u003c\/em\u003e\u003c\/p\u003e\u003c\/blockquote\u003e\n\u003cp\u003eConvolutional neural networks have been the driving force behind almost every major breakthrough in computer vision — \u003cstrong\u003ebut what if they’ve been holding us back all along?\u003c\/strong\u003e\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "https:\/\/vickiiimu.github.io\/my-site\/posts\/vit\/",
    "author" : {
      "@type": "Person",
      "name": "Vicki Mu"
    },
    "copyrightYear": "2025",
    "dateCreated": "2025-06-26T00:00:00\u002b00:00",
    "datePublished": "2025-06-26T00:00:00\u002b00:00",
    
    "dateModified": "2025-06-26T00:00:00\u002b00:00",
    
    "keywords": ["deep learning","transformers","computer vision"],
    
    "mainEntityOfPage": "true",
    "wordCount": "3228"
  }]
  </script>


  
  
  <meta name="author" content="Vicki Mu" />
  
  
  
  
  <link href="https://twitter.com/burufugu" rel="me" />
  
  
  
  
  <link href="https://github.com/nunocoracao/blowfish" rel="me" />
  
  
  
  
  

<script src="/my-site/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>






















  
  

  <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=G-PEDMYR1V0K"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PEDMYR1V0K');
</script>





  
  
  
  
  <meta name="theme-color"/>
  
  

  
  
</head>
<body
    class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
    <div id="the-top" class="absolute flex self-center">
      <a
        class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a
      >
    </div>
    
    
      <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 pl-[24px] pr-[24px] z-index-100">
  <div
    id="menu-blur"
    class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div>
  <div class="relative max-w-[64rem] ml-auto mr-auto">
    <div class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3 padding-main-menu">
    
    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/my-site/" class="text-base font-medium text-gray-500 hover:text-gray-900">For Later Analysis</a>
            

        </nav>
        <nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12">

            
            
            
  <a
  href="/my-site/about/"
  
  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
  
  <p class="text-base font-medium" title="About Me">
    About Me
  </p>
</a>



            
            
  <a
  href="/my-site/posts/"
  
  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
  
  <p class="text-base font-medium" title="Posts">
    Posts
  </p>
</a>



            
            
  <a
  href="https://x.com/vickiiimu"
  target="_blank"
  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
  
    <span >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>


    </span>
  
  <p class="text-base font-medium" title="">
    
  </p>
</a>



            
            
  <a
  href="https://github.com/vickiiimu"
  target="_blank"
  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
  
    <span >
      

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>


    </span>
  
  <p class="text-base font-medium" title="">
    
  </p>
</a>



            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            
            <div
                class=" flex items-center">
                <button id="appearance-switcher" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400">
                    <div class="flex items-center justify-center dark:hidden">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                    </div>
                    <div class="items-center justify-center hidden dark:flex">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                    </div>
                </button>
            </div>
            

        </nav>
        <div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            
            <button id="appearance-switcher-mobile" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1">
                <div class="flex items-center justify-center dark:hidden">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                </div>
                <div class="items-center justify-center hidden dark:flex">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                </div>
            </button>
            

        </div>
    </div>
    <div class="-my-2 md:hidden">

        <label id="menu-button" class="block">
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50 padding-top-[5px]">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li id="menu-close-button">
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                    
  <li class="mt-1">
  <a
    href="/my-site/about/"
    
    class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-bg font-bg" title="About Me">
      About Me
    </p>
  </a>
</li>




                    

                    
  <li class="mt-1">
  <a
    href="/my-site/posts/"
    
    class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-bg font-bg" title="Posts">
      Posts
    </p>
  </a>
</li>




                    

                    
  <li class="mt-1">
  <a
    href="https://x.com/vickiiimu"
    
      target="_blank"
    
    class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
      <div >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>


      </div>
    
    <p class="text-bg font-bg" title="">
      
    </p>
  </a>
</li>




                    

                    
  <li class="mt-1">
  <a
    href="https://github.com/vickiiimu"
    
      target="_blank"
    
    class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
      <div >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>


      </div>
    
    <p class="text-bg font-bg" title="">
      
    </p>
  </a>
</li>




                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>




<script>
    (function () {
        var $mainmenu = $('.main-menu');
        var path = window.location.pathname;
        $mainmenu.find('a[href="' + path + '"]').each(function (i, e) {
            $(e).children('p').addClass('active');
        });
    })();
</script>


  </div>
</div>


<script
  type="text/javascript"
  src="/my-site/js/background-blur.min.4812321520cc2e4cd52bf9137015a55bb6c0b5e66a6b2c96b8654c0ec99be59e8e6425dd838a6fd9bac54000586f37f4a9680bbefaa2aee4601a69e188005cf6.js"
  integrity="sha512-SBIyFSDMLkzVK/kTcBWlW7bAteZqayyWuGVMDsmb5Z6OZCXdg4pv2brFQABYbzf0qWgLvvqiruRgGmnhiABc9g=="
  data-target-id="menu-blur"></script>

    
    <div class="relative flex flex-col grow">
      <main id="main-content" class="grow">
        
  


  <article>
    
      
      
      
      
        


<div id="hero" class="h-[150px] md:h-[200px]"></div>



    
    <div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom"
    style="background-image:url(/my-site/posts/vit/background_hu_76627b3d3930dddc.jpg);">
    


    <div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal">
    </div>
    <div
        class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal">
    </div>
</div>

<div id="background-blur" class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div>


<script type="text/javascript" src="/my-site/js/background-blur.min.4812321520cc2e4cd52bf9137015a55bb6c0b5e66a6b2c96b8654c0ec99be59e8e6425dd838a6fd9bac54000586f37f4a9680bbefaa2aee4601a69e188005cf6.js" integrity="sha512-SBIyFSDMLkzVK/kTcBWlW7bAteZqayyWuGVMDsmb5Z6OZCXdg4pv2brFQABYbzf0qWgLvvqiruRgGmnhiABc9g==" data-target-id="background-blur"></script>

      
    


    <header id="single_header" class="mt-5 max-w-prose">
      
        <ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden">
  
  
    
  
    
  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/my-site/"
      >Vicki Mu</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="inline">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/my-site/posts/"
      >Posts</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

  
  <li class="hidden">
    <a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href="/my-site/posts/vit/"
      >From Pixels to Predictions: Building a Transformer for Images</a
    ><span class="px-1 text-primary-500">/</span>
  </li>

</ol>


      
      <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        From Pixels to Predictions: Building a Transformer for Images
      </h1>
      <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
        





  
  



  

  
  
  

  
  
    
  

  

  

  

  
    
  

  

  

  

  
    
  

  
    
  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <span title="Reading time">16 mins</span><span class="px-2 text-primary-500">&middot;</span>


<span class="mb-[2px]">
  <a
    href="https://github.com/vickiiimu/vit-cifar10-tutorial"
    class="text-lg hover:text-primary-500"
    rel="noopener noreferrer"
    target="_blank"
    title="Edit content"
    ><span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 512 512"><path fill="currentColor" d="M441 58.9L453.1 71c9.4 9.4 9.4 24.6 0 33.9L424 134.1 377.9 88 407 58.9c9.4-9.4 24.6-9.4 33.9 0zM209.8 256.2L344 121.9 390.1 168 255.8 302.2c-2.9 2.9-6.5 5-10.4 6.1l-58.5 16.7 16.7-58.5c1.1-3.9 3.2-7.5 6.1-10.4zM373.1 25L175.8 222.2c-8.7 8.7-15 19.4-18.3 31.1l-28.6 100c-2.4 8.4-.1 17.4 6.1 23.6s15.2 8.5 23.6 6.1l100-28.6c11.8-3.4 22.5-9.7 31.1-18.3L487 138.9c28.1-28.1 28.1-73.7 0-101.8L474.9 25C446.8-3.1 401.2-3.1 373.1 25zM88 64C39.4 64 0 103.4 0 152V424c0 48.6 39.4 88 88 88H360c48.6 0 88-39.4 88-88V312c0-13.3-10.7-24-24-24s-24 10.7-24 24V424c0 22.1-17.9 40-40 40H88c-22.1 0-40-17.9-40-40V152c0-22.1 17.9-40 40-40H200c13.3 0 24-10.7 24-24s-10.7-24-24-24H88z"/></svg>
  </span>

</span></a
  >
</span><span class="px-2 text-primary-500">&middot;</span>



<script
  type="text/javascript"
  src="/my-site/js/zen-mode.min.2a0a1375509c03daf41c53a9831aa2d560895b7a2528c3eb3b56e7b11a135c7529e49727162a053b6193be8340270a8342f42c69f3d7e262ae54bc248eaac8f5.js"
  integrity="sha512-KgoTdVCcA9r0HFOpgxqi1WCJW3olKMPrO1bnsRoTXHUp5JcnFioFO2GTvoNAJwqDQvQsafPX4mKuVLwkjqrI9Q=="></script>

<span class="mb-[2px]">
  <span
    id="zen-mode-button"
    class="text-lg hover:text-primary-500"
    title="Enable zen mode"
    data-title-i18n-disable="Enable zen mode"
    data-title-i18n-enable="Disable zen mode">
    <span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="50px" height="50px">
    <path fill="currentColor" d="M 12.980469 4 C 9.1204688 4 5.9804688 7.14 5.9804688 11 L 6 26 L 9.9804688 26 L 9.9804688 11 C 9.9804688 9.35 11.320469 8 12.980469 8 L 40.019531 8 C 41.679531 8 43.019531 9.35 43.019531 11 L 43.019531 39 C 43.019531 40.65 41.679531 42 40.019531 42 L 29 42 C 29 43.54 28.420938 44.94 27.460938 46 L 40.019531 46 C 43.879531 46 47.019531 42.86 47.019531 39 L 47.019531 11 C 47.019531 7.14 43.879531 4 40.019531 4 L 12.980469 4 z M 7 28 C 4.794 28 3 29.794 3 32 L 3 42 C 3 44.206 4.794 46 7 46 L 23 46 C 25.206 46 27 44.206 27 42 L 27 32 C 27 29.794 25.206 28 23 28 L 7 28 z M 7 32 L 23 32 L 23.001953 42 L 7 42 L 7 32 z"/>
</svg>
  </span>

</span>
  </span>
</span>

    

    
    
  </div>

  
    <div class="flex flex-row flex-wrap items-center">
      
        
          
        
      
        
      
        
      
        
      
    </div>
  

  
  
    <div class="flex flex-row flex-wrap items-center">
      
        
      
        
          
            
              <span
                class="mr-2 margin-top-[0.5rem]"
                onclick="window.open(&#34;/my-site/categories/recreated-from-scratch/&#34;,'_self');return false;">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Recreated From Scratch
  </span>
</span>

              </span>
            
          
        
      
        
      
        
          
            
              <span
                class="mr-2 margin-top-[0.5rem]"
                onclick="window.open(&#34;/my-site/tags/deep-learning/&#34;,'_self');return false;">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Deep Learning
  </span>
</span>

              </span>
            
              <span
                class="mr-2 margin-top-[0.5rem]"
                onclick="window.open(&#34;/my-site/tags/transformers/&#34;,'_self');return false;">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Transformers
  </span>
</span>

              </span>
            
              <span
                class="mr-2 margin-top-[0.5rem]"
                onclick="window.open(&#34;/my-site/tags/computer-vision/&#34;,'_self');return false;">
                <span class="flex cursor-pointer">
  <span
    class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">
    Computer Vision
  </span>
</span>

              </span>
            
          
        
      
    </div>
  

  
  



      </div>

      
      
      
      
      

      

      

        
          
          
<div class="flex author">
  
    
    
      
    
    
      
        
      
      <img
        class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4"
        width="96"
        height="96"
        alt="Vicki Mu"
        src="/my-site/profile_hu_64024b71352581b9.png">
    
  
  <div class="place-self-center">
    
      <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
        Author
      </div>
      <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
        Vicki Mu
      </div>
    
    
      <div class="text-sm text-neutral-700 dark:text-neutral-400">I’m a student at MIT exploring the edges of intelligence—building models, recreating ideas, and writing to figure out what I think.</div>
    
    <div class="text-2xl sm:text-lg">
  <div class="flex flex-wrap text-neutral-400 dark:text-neutral-500">
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://twitter.com/burufugu"
          target="_blank"
          aria-label="X-Twitter"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>

</span></a
        >
      
    
      
        <a
          class="px-1 hover:text-primary-700 dark:hover:text-primary-400"
          href="https://github.com/nunocoracao/blowfish"
          target="_blank"
          aria-label="Github"
          rel="me noopener noreferrer"
          ><span class="inline-block align-text-bottom">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>

  </span>

</span></a
        >
      
    
  </div>

</div>
  </div>
</div>

        

        

        
          <div class="mb-5"></div>
        

      

    </header>

    <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
      
        <div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8">
          <div
            class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]
            
            ">
            
              <details
  open
  id="TOCView"
  class="toc-right mt-0 overflow-y-auto overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block">
  <summary
    class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#1-background-and-intuition">1. Background and Intuition</a>
      <ul>
        <li><a href="#11-from-recurrent-models-to-the-rise-of-transformers-in-nlp">1.1 From Recurrent Models to the Rise of Transformers (in NLP)</a></li>
        <li><a href="#12-can-attention-replace-convolution-the-shift-to-vision">1.2 Can Attention Replace Convolution? The Shift to Vision</a></li>
      </ul>
    </li>
    <li><a href="#2-how-vision-transformers-work">2. How Vision Transformers Work</a>
      <ul>
        <li><a href="#21-patch-embedding">2.1 Patch Embedding</a></li>
        <li><a href="#22-class-token-and-positional-embeddings">2.2 Class Token and Positional Embeddings</a></li>
        <li><a href="#23-multi-head-self-attention-mhsa">2.3 Multi-Head Self-Attention (MHSA)</a></li>
        <li><a href="#24-transformer-encoder">2.4 Transformer Encoder</a></li>
        <li><a href="#25-classification-head">2.5 Classification Head</a></li>
      </ul>
    </li>
    <li><a href="#3-implementation-walkthrough">3. Implementation Walkthrough</a>
      <ul>
        <li><a href="#31-patch-embedding">3.1 Patch Embedding</a></li>
        <li><a href="#32-class-token-and-positional-embeddings">3.2 Class Token and Positional Embeddings</a></li>
        <li><a href="#33-multi-head-self-attention">3.3 Multi-Head Self-Attention</a></li>
        <li><a href="#34-transformer-encoder">3.4 Transformer Encoder</a></li>
        <li><a href="#35-putting-it-all-together">3.5 Putting It All Together</a></li>
      </ul>
    </li>
    <li><a href="#4-training-the-vit">4. Training the ViT</a>
      <ul>
        <li><a href="#41-dataset-cifar-10">4.1 Dataset: CIFAR-10</a></li>
        <li><a href="#42-model-setup-adapting-vit-for-cifar-10">4.2 Model Setup: Adapting ViT for CIFAR-10</a></li>
        <li><a href="#43-training-setup">4.3 Training Setup</a></li>
        <li><a href="#43-results">4.3 Results</a></li>
      </ul>
    </li>
    <li><a href="#5-limitations-and-extensions">5. Limitations and Extensions</a>
      <ul>
        <li><a href="#51-limitations">5.1 Limitations</a></li>
        <li><a href="#52-extensions-and-improvements">5.2 Extensions and Improvements</a></li>
      </ul>
    </li>
    <li><a href="#6-github--colab">6. GitHub + Colab</a>
      <ul>
        <li><a href="#view-the-github">View the <a href="https://github.com/vickiiimu/vit-cifar10-tutorial"><strong>Github</strong></a></a></li>
        <li><a href="#open-in-colab">Open in <a href="https://colab.research.google.com/drive/1vn57VeLHweWZCiEiJoc39q_NYUKuSXrc?usp=sharing"><strong>Colab</strong></a></a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#7-conclusion">7. Conclusion</a>
      <ul>
        <li><a href="#feedback-welcome">Feedback welcome</a></li>
        <li><a href="#further-reading">Further reading</a></li>
        <li><a href="#about-me">About Me</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</details>
<details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden">
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">
    Table of Contents
  </summary>
  <div
    class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#1-background-and-intuition">1. Background and Intuition</a>
      <ul>
        <li><a href="#11-from-recurrent-models-to-the-rise-of-transformers-in-nlp">1.1 From Recurrent Models to the Rise of Transformers (in NLP)</a></li>
        <li><a href="#12-can-attention-replace-convolution-the-shift-to-vision">1.2 Can Attention Replace Convolution? The Shift to Vision</a></li>
      </ul>
    </li>
    <li><a href="#2-how-vision-transformers-work">2. How Vision Transformers Work</a>
      <ul>
        <li><a href="#21-patch-embedding">2.1 Patch Embedding</a></li>
        <li><a href="#22-class-token-and-positional-embeddings">2.2 Class Token and Positional Embeddings</a></li>
        <li><a href="#23-multi-head-self-attention-mhsa">2.3 Multi-Head Self-Attention (MHSA)</a></li>
        <li><a href="#24-transformer-encoder">2.4 Transformer Encoder</a></li>
        <li><a href="#25-classification-head">2.5 Classification Head</a></li>
      </ul>
    </li>
    <li><a href="#3-implementation-walkthrough">3. Implementation Walkthrough</a>
      <ul>
        <li><a href="#31-patch-embedding">3.1 Patch Embedding</a></li>
        <li><a href="#32-class-token-and-positional-embeddings">3.2 Class Token and Positional Embeddings</a></li>
        <li><a href="#33-multi-head-self-attention">3.3 Multi-Head Self-Attention</a></li>
        <li><a href="#34-transformer-encoder">3.4 Transformer Encoder</a></li>
        <li><a href="#35-putting-it-all-together">3.5 Putting It All Together</a></li>
      </ul>
    </li>
    <li><a href="#4-training-the-vit">4. Training the ViT</a>
      <ul>
        <li><a href="#41-dataset-cifar-10">4.1 Dataset: CIFAR-10</a></li>
        <li><a href="#42-model-setup-adapting-vit-for-cifar-10">4.2 Model Setup: Adapting ViT for CIFAR-10</a></li>
        <li><a href="#43-training-setup">4.3 Training Setup</a></li>
        <li><a href="#43-results">4.3 Results</a></li>
      </ul>
    </li>
    <li><a href="#5-limitations-and-extensions">5. Limitations and Extensions</a>
      <ul>
        <li><a href="#51-limitations">5.1 Limitations</a></li>
        <li><a href="#52-extensions-and-improvements">5.2 Extensions and Improvements</a></li>
      </ul>
    </li>
    <li><a href="#6-github--colab">6. GitHub + Colab</a>
      <ul>
        <li><a href="#view-the-github">View the <a href="https://github.com/vickiiimu/vit-cifar10-tutorial"><strong>Github</strong></a></a></li>
        <li><a href="#open-in-colab">Open in <a href="https://colab.research.google.com/drive/1vn57VeLHweWZCiEiJoc39q_NYUKuSXrc?usp=sharing"><strong>Colab</strong></a></a></li>
        <li><a href="#installation">Installation</a></li>
      </ul>
    </li>
    <li><a href="#7-conclusion">7. Conclusion</a>
      <ul>
        <li><a href="#feedback-welcome">Feedback welcome</a></li>
        <li><a href="#further-reading">Further reading</a></li>
        <li><a href="#about-me">About Me</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</details>


<script>
  (function () {
    'use strict'

    const SCROLL_OFFSET_RATIO = 0.33
    const TOC_SELECTOR = '#TableOfContents'
    const ANCHOR_SELECTOR = '.anchor'
    const TOC_LINK_SELECTOR = 'a[href^="#"]'
    const NESTED_LIST_SELECTOR = 'li ul'
    const ACTIVE_CLASS = 'active'

    function getActiveAnchorId(anchors, offsetRatio) {
      const threshold = window.scrollY + window.innerHeight * offsetRatio
      const tocLinks = [...document.querySelectorAll('#TableOfContents a[href^="#"]')]
      const tocIds = new Set(tocLinks.map(link => link.getAttribute('href').substring(1)))

      for (let i = anchors.length - 1; i >= 0; i--) {
        const top = anchors[i].getBoundingClientRect().top + window.scrollY
        if (top <= threshold && tocIds.has(anchors[i].id)) {
          return anchors[i].id
        }
      }
      return anchors.find(anchor => tocIds.has(anchor.id))?.id || ''
    }

    function updateTOC({ toc, anchors, links, scrollOffset, collapseInactive }) {
      const activeId = getActiveAnchorId(anchors, scrollOffset)
      if (!activeId) return

      links.forEach(link => {
        const isActive = link.getAttribute('href') === `#${activeId}`
        link.classList.toggle(ACTIVE_CLASS, isActive)

        if (collapseInactive) {
          const ul = link.closest('li')?.querySelector('ul')
          if (ul) ul.style.display = isActive ? '' : 'none'
        }
      })

      if (collapseInactive) {
        const activeLink = toc.querySelector(`a[href="#${CSS.escape(activeId)}"]`)
        let el = activeLink
        while (el && el !== toc) {
          if (el.tagName === 'UL') el.style.display = ''
          if (el.tagName === 'LI') el.querySelector('ul')?.style.setProperty('display', '')
          el = el.parentElement
        }
      }
    }

    function initTOC() {
      const toc = document.querySelector(TOC_SELECTOR)
      if (!toc) return

      const collapseInactive = false
      const anchors = [...document.querySelectorAll(ANCHOR_SELECTOR)]
      const links = [...toc.querySelectorAll(TOC_LINK_SELECTOR)]

      if (collapseInactive) {
        toc.querySelectorAll(NESTED_LIST_SELECTOR).forEach(ul => ul.style.display = 'none')
      }

      const config = {
        toc,
        anchors,
        links,
        scrollOffset: SCROLL_OFFSET_RATIO,
        collapseInactive
      }

      window.addEventListener('scroll', () => updateTOC(config), { passive: true })
      window.addEventListener('hashchange', () => updateTOC(config), { passive: true })

      updateTOC(config)
    }

    document.readyState === 'loading'
      ? document.addEventListener('DOMContentLoaded', initTOC)
      : initTOC()
  })()
</script>


            
            
          </div>
        </div>
      


      <div class="min-w-0 min-h-0 max-w-fit">
        
  <details
    class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 margin-left-[0px]"
    >
    
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">
    Recreated from Scratch -
    This article is part of a series.
  </summary>
  
  
    
      <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        Part 1:
        This Article
      </div>
    
  


  </details>




        <div class="article-content max-w-prose mb-20">
          <blockquote>
<p><em>Originally published on <a
  href="https://pub.towardsai.net/from-pixels-to-predictions-building-a-transformer-for-images-fea5a4f64816"
    target="_blank"
  >Medium</a>
.</em></p></blockquote>
<p>Convolutional neural networks have been the driving force behind almost every major breakthrough in computer vision — <strong>but what if they’ve been holding us back all along?</strong></p>
<p>In 2020, a team of researchers at Google asked the bold question: <em>Can we throw out convolutions entirely and still build world-class image models?</em><br>
Their answer — <strong>the Vision Transformer (ViT)</strong>— sparked a new era in deep learning.</p>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/featured-ViTarch.png" alt="Diagram showing the architecture of a Vision Transformer (ViT). An input image is divided into patches, linearly embedded with positional information, and passed through a stack of transformer encoder blocks. The encoder includes multi-head self-attention, normalization, and MLP layers, with output passed to a classification head." />
  <figcaption><strong>Figure 1</strong>: Vision Transformer (ViT) architecture. Source: Dosovitskiy et al., 2020 (<a
  href="https://arxiv.org/abs/2010.11929"
    target="_blank"
  >arXiv:2010.11929</a>
)</figcaption>
  
  </figure>
<p>I&rsquo;m an undergraduate student at MIT with an interest in computer vision and generative models, and I recently implemented a Vision Transformer from scratch to better understand its architecture. <em>This post is a distilled guide to that process, blending theory, visuals and code.</em></p>
<p>We&rsquo;ll walk through how ViTs turn images into sequences, how attention works in this context, and how it compares to the CNNs you&rsquo;re used to. By the end, you’ll have a <strong>working ViT in PyTorch</strong> and a <strong>much deeper understanding of how it all fits together</strong>.</p>
<hr>

<h2 class="relative group">1. Background and Intuition 
    <div id="1-background-and-intuition" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#1-background-and-intuition" aria-label="Anchor">#</a>
    </span>        
    
</h2>

<h3 class="relative group">1.1 From Recurrent Models to the Rise of Transformers (in NLP) 
    <div id="11-from-recurrent-models-to-the-rise-of-transformers-in-nlp" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#11-from-recurrent-models-to-the-rise-of-transformers-in-nlp" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Before 2017, NLP was ruled by RNNs and LSTMs: models that powered everything from <em>machine translation</em> to <em>language modeling</em>. But despite their early success, they had fundamental limitations. Because they processed sequences one token at a time, training couldn’t be parallelized. And as sequences got longer, they struggled to retain information from earlier tokens. These bottlenecks made it difficult to scale up, especially for tasks that required a deep, global understanding of language.</p>
<p>In 2017, researchers at Google made a breakthrough in their paper <a
  href="https://arxiv.org/abs/1706.03762"
    target="_blank"
  ><em>Attention Is All You Need</em></a>
. It proposed a new architecture — the <strong>Transformer</strong> — built around a simple but powerful idea: <strong>self-attention</strong>. Instead of processing tokens one at a time, self-attention allows each token to directly consider every other token in the sequence.</p>
<blockquote>
<p><em>Put simply, each word learns to ask questions (<strong>queries</strong>), decide what to listen to (<strong>keys</strong>), and gather relevant information (<strong>values</strong>).</em></p></blockquote>
<p>This mechanism eliminated the need for recurrence and fixed step order, sidestepping the main weaknesses of RNNs.</p>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/RNN-vs-Transformer.jpg" alt="Diagram comparing RNNs and Transformers. On the left, RNNs process inputs sequentially from left to right, passing hidden states forward through time steps. On the right, a Transformer encoder processes all inputs simultaneously, with each input connected to all others using weighted attention lines of varying thickness. Arrows and labels highlight the difference in processing order and parallelism." />
  <figcaption><strong>Figure 2</strong>: RNNs handle inputs sequentially, while Transformers attend to all tokens in parallel. Line thickness represents attention strength. Source: author.</figcaption>
  
  </figure>
<p>Within just two years, <strong>Transformer architecture</strong> completely took over NLP.<br>
It proved more efficient, easier to scale, and better at modeling long-range dependencies than any of its predecessors. Transformers quickly became the backbone of major breakthrough models: <strong>BERT</strong> (for bi-directional context), <strong>GPT</strong> (for generative, causal language modeling), and <strong>T5</strong> (for sequence to sequence tasks).</p>
<p><strong>RNNs were replaced by attention in NLP — but what about computer vision?</strong><br>
At the time, CNNs dominated the field, but they came with their own set of limitations. Convolutions are inherently local, making it difficult for CNNs to capture long-range dependencies. They also rely heavily on spatial prior and careful feature engineering.</p>
<p>So the natural next question emerged: <em>if attention could replace recurrence… could it replace convolution too?</em></p>

<h3 class="relative group">1.2 Can Attention Replace Convolution? The Shift to Vision 
    <div id="12-can-attention-replace-convolution-the-shift-to-vision" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#12-can-attention-replace-convolution-the-shift-to-vision" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>In 2020, Dosovitskiy et al. introduced the <strong>Vision Transformer (ViT)</strong> in their paper <a
  href="https://arxiv.org/abs/2010.11929"
    target="_blank"
  ><em>An Image Is Worth 16×16 Words</em></a>
. They proposed a bold idea: <em>what if we treated an image like a sentence?</em></p>
<p>Instead of relying on convolutional filters, they divided images into patches and fed them into a standard transformer. While early ViTs needed massive datasets to compete with CNNs, <strong>the approach proved that attention-based models could work for vision</strong>—not just language.</p>
<p>Since its release, the Vision Transformer has sparked a wave of improvements:</p>
<ul>
<li><strong>DeiT</strong> introduced smarter training strategies to reduce ViT’s reliance on huge datasets</li>
<li><strong>Swin Transformer</strong> added hierarchical structure to better handle local spatial patterns</li>
<li><strong>DINO</strong> and <strong>DINOv2</strong> showed that ViTs could learn rich visual representations <em>without any labels at all</em>—unlocking powerful self-supervised features for downstream tasks.</li>
</ul>
<p>What began as a bold experiment has now become a core building block in modern computer vision.</p>
<hr>

<h2 class="relative group">2. How Vision Transformers Work 
    <div id="2-how-vision-transformers-work" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#2-how-vision-transformers-work" aria-label="Anchor">#</a>
    </span>        
    
</h2>

<h3 class="relative group">2.1 Patch Embedding 
    <div id="21-patch-embedding" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#21-patch-embedding" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Transformers were originally designed to process sequences, like sentences made out of word tokens. But images are 2D grids of pixels, not 1D sequences. So how do we feed an image into a transformer?</p>
<p>The Vision Transformer solves this by dividing the image into <strong>non-overlapping square patches</strong> (e.g. 16x16 pixels). Each patch is then flattened into a 1D vector and linearly projected into a fixed-size embedding—just like token embeddings in NLP.</p>
<p>For example:</p>
<ul>
<li>A 224x224 image with 16x16 patches produces (224/16)² = 196 patches.</li>
<li>Each patch is of shape 3x16x16 (RGB).</li>
<li>Each flattened patch becomes a 768-dim vector (common for ViT-Base).</li>
</ul>
<p>Instead of a sentence of words, ViT sees a sequence of image patch embeddings.</p>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/Patchembedding.jpg" alt="Patch embedding diagram from ViT paper" />
  <figcaption><strong>Figure 3</strong>: Patch embedding. Source: <a
  href="https://arxiv.org/abs/2010.11929"
    target="_blank"
  >Dosovitskiy et al., 2020</a></figcaption>
  
  </figure>
<blockquote>
<p><strong>Analogy</strong>: Just like a tokenizer turns a sentence into a sequence of word embeddings, the ViT turns an image into a sequence of patch embeddings.</p></blockquote>

<h3 class="relative group">2.2 Class Token and Positional Embeddings 
    <div id="22-class-token-and-positional-embeddings" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#22-class-token-and-positional-embeddings" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Transformers need two extra ingredients to work properly with image sequences:</p>
<ol>
<li>a <strong>[CLS] token</strong> to aggregate global information, and</li>
<li><strong>positional embeddings</strong> to encode spatial structure.</li>
</ol>
<p>In ViT, a special learnable token is prepended to the input sequence. During self-attention, this token attends to every patch—and becomes the representation used for final classification.</p>
<p>Transformers are <strong>permutation-invariant</strong>—they don’t inherently understand order. To give the model spatial awareness, we add a unique positional embedding to each token in the sequence.</p>
<p>Both the <code>[CLS]</code> token and positional embeddings are <strong>learned parameters</strong>, updated during training.</p>

<h3 class="relative group">2.3 Multi-Head Self-Attention (MHSA) 
    <div id="23-multi-head-self-attention-mhsa" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#23-multi-head-self-attention-mhsa" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>At the heart of the Vision Transformer is the <strong>multi-head self-attention mechanism</strong>—the part that allows the model to understand how image patches relate to each other, regardless of spatial distance.</p>
<p>Instead of using one attention function, MHSA splits the input into <strong>multiple “heads”</strong>. Each head learns to focus on different aspects of the input—some might focus on edges, others on texture, others on spatial layout. Their outputs are then concatenated and projected back into the original embedding space.</p>
<p><strong>How it works, step by step:</strong></p>
<ul>
<li>The input sequence of tokens (shape <code>[B, N, D]</code>) is linearly projected into: <strong>Queries</strong> <code>Q</code>, <strong>Keys</strong> <code>K</code>, and <strong>Values</strong> <code>V</code>.</li>
<li>Each attention head computes:</li>
</ul>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/sdp.png" alt="Attention equation visual" />
  <figcaption>Equation 1: Scaled Dot-Product Attention</figcaption>
  
  </figure>
<ul>
<li>Multiple heads run in parallel, and their outputs are concatenated and linearly projected back.</li>
</ul>
<p><strong>Why “multi-head”?</strong><br>
Each head attends to different parts of the sequence. This allows the model to understand complex relationships in parallel—not just spatial proximity, but also semantic structure.</p>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/multihead.png" alt="Two diagrams. On the left: Scaled Dot-Product Attention showing the flow Q -&gt; K -&gt; V through MatMul, Scale, Mask, Softmax, and another MatMul. On the right: Multi-Head Attention with multiple scaled dot-product attention heads, whose outputs are concatenated and passed through a linear layer." />
  <figcaption><strong>Figure 4</strong>: The left shows how attention scores are computed using queries (Q), keys (K), and values (V). The right illustrates how multiple attention &lsquo;heads&rsquo; capture diverse representations in parallel. Source: Vaswani et al. (<a
  href="https://arxiv.org/abs/1706.03762"
    target="_blank"
  >arXiv:1706.03762</a>
)</figcaption>
  
  </figure>

<h3 class="relative group">2.4 Transformer Encoder 
    <div id="24-transformer-encoder" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#24-transformer-encoder" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Once we have self-attention, we wrap it inside a larger unit: the <strong>Transformer block</strong>. This block is the fundamental building unit of ViTs (and NLP Transformers too). It combines:</p>
<ul>
<li><strong>LayerNorm → Multi-Head Attention → Residual Connection</strong></li>
<li><strong>LayerNorm → MLP (Feedforward Network) → Residual Connection</strong></li>
</ul>
<p>Each block enables the model to <strong>attend globally and transform features</strong> across layers while maintaining stability with normalization and residuals.</p>
<p><strong>What’s inside a ViT Transformer block:</strong></p>
<ol>
<li><strong>LayerNorm</strong> before attention (called <em>pre-norm</em>).</li>
<li><strong>Multi-head self-attention</strong> applied to the normalized input.</li>
<li>A <strong>residual connection</strong> adds the attention output back.</li>
<li>Another <strong>LayerNorm</strong>, followed by a small <strong>MLP</strong>.</li>
<li>Another <strong>residual connection</strong> adds the MLP output.</li>
</ol>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/transformer.png" alt="Diagram of a Vision Transformer block. The input embedded patches go through LayerNorm, Multi-Head Attention, residual connection, then LayerNorm, MLP, and another residual connection." />
  <figcaption><strong>Figure 5</strong>: ViT encoder block with attention, MLP, and residuals. Source: author</figcaption>
  
  </figure>
<p>This structure repeats across all transformer layers (e.g., 12 layers in ViT-Base).</p>

<h3 class="relative group">2.5 Classification Head 
    <div id="25-classification-head" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#25-classification-head" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>After processing the input through multiple Transformer blocks, the model needs a way to produce a final prediction. In Vision Transformers, this is handled by the <strong>classification head</strong>.</p>
<p>During the embedding step, we added a special <strong>[CLS] token</strong> at the beginning of the sequence. Just like in BERT, this token is intended to <strong>aggregate information</strong> from all the image patches through self-attention. After passing through all Transformer layers, the <strong>final embedding of the [CLS] token</strong> is used as a summary representation of the entire image.</p>
<hr>

<h2 class="relative group">3. Implementation Walkthrough 
    <div id="3-implementation-walkthrough" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#3-implementation-walkthrough" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>All core modules—patch embedding, MHSA, encoder blocks—are implemented from scratch. No <code>timm</code> shortcuts.</p>

<h3 class="relative group">3.1 Patch Embedding 
    <div id="31-patch-embedding" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#31-patch-embedding" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>To convert image patches into a sequence of embeddings, we use a clever trick. Instead of writing a manual for-loop to extract and flatten patches, we can use a <code>Conv2d</code> layer with:</p>
<ul>
<li><code>kernel_size = patch_size</code></li>
<li><code>stride = patch_size</code></li>
</ul>
<p>This extracts <strong>non-overlapping patches</strong> and applies a <strong>learned linear projection</strong>—all in a single operation. It’s clean, efficient, and easy to <strong>backpropagate</strong> through.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">PatchEmbed</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_size</span> <span class="o">=</span> <span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">in_chans</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">768</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">img_size</span> <span class="o">=</span> <span class="n">img_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">in_chans</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">embed_dim</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">patch_size</span><span class="p">,</span> 
</span></span><span class="line"><span class="cl">            <span class="n">stride</span> <span class="o">=</span> <span class="n">patch_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># x shape: [B, 3, 224, 224]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># [B, emdbed_dim, H/patch, W/patch]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># [B, emdbed_dim, num_patches]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># [B, num_patches, embed_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div>
<h3 class="relative group">3.2 Class Token and Positional Embeddings 
    <div id="32-class-token-and-positional-embeddings" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#32-class-token-and-positional-embeddings" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Here we define a <code>ViTEmbed</code> module that:</p>
<ul>
<li>Prepends a learnable <code>[CLS]</code> token to the sequence</li>
<li>Adds a learnable positional embedding to each token (including <code>[CLS]</code>)</li>
</ul>
<p>This produces a sequence shaped <code>[B, num_patches + 1, embed_dim]</code> — ready for the transformer encoder.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">ViTEmbed</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_patches</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># Learnable [CLS] token (1 per model)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>  <span class="c1"># [1, 1, D]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Learnable positional embeddings (1 per token, including CLS)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>  <span class="c1"># [1, N+1, D]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Expand [CLS] token to match batch size</span>
</span></span><span class="line"><span class="cl">        <span class="n">cls_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, 1, D]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Prepend CLS token to patch embeddings</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">cls_tokens</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, N+1, D]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Add positional embeddings</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span>  <span class="c1"># [B, N+1, D]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div>
<h3 class="relative group">3.3 Multi-Head Self-Attention 
    <div id="33-multi-head-self-attention" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#33-multi-head-self-attention" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Let’s implement one of the most important parts of the Vision Transformer: <strong>multi-head self-attention</strong>.</p>
<p>Each input token is linearly projected into a <strong>query (Q)</strong>, <strong>key (K)</strong>, and <strong>value (V)</strong> vector. Attention is computed in parallel across multiple heads, then concatenated and projected back to the original embedding dimension.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MyMultiheadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;embed_dim must be divisible by num_heads&#34;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Learnable projections for Q, K, V</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Final output projection</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># [batch, seq_len, embed_dim]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Project input into Q, K, V</span>
</span></span><span class="line"><span class="cl">        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Reshape into heads: [B, num_heads, T, head_dim]</span>
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">Q</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">K</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">V</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Scaled dot-product attention</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [B, heads, T, T]</span>
</span></span><span class="line"><span class="cl">        <span class="n">scores</span> <span class="o">/=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">**</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Apply attention to values</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>  <span class="c1"># [B, heads, T, head_dim]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Recombine heads</span>
</span></span><span class="line"><span class="cl">        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Final linear projection</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</span></span></code></pre></div>
<h3 class="relative group">3.4 Transformer Encoder 
    <div id="34-transformer-encoder" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#34-transformer-encoder" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>We now wrap everything together in a <strong>Transformer block</strong> — a modular unit that stacks self-attention and MLP layers with residual connections. This design lets the model reason globally (through self-attention) and then transform those representations (through the MLP), all while preserving stability via skip connections.</p>
<p>In this implementation:</p>
<ul>
<li>We use our own <code>MyMultiheadAttention</code> <strong>class</strong> from earlier to demystify how attention works under the hood.</li>
<li>In practice, you can use PyTorch’s built-in <code>nn.MultiheadAttention</code> for convenience and efficiency.</li>
<li>We apply <strong>LayerNorm before</strong> both the attention and MLP layers (a “pre-norm” design).</li>
<li>The <code>mlp_ratio</code> controls the width of the MLP’s hidden layer (usually 3–4× wider than the embedding dimension).</li>
</ul>
<p>Let’s build the full Transformer block:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="mf">4.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
</span></span><span class="line"><span class="cl">            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="n">mlp_ratio</span><span class="p">),</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># Self-attention with residual connection</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># MLP with residual connection</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">x</span>
</span></span></code></pre></div>
<h3 class="relative group">3.5 Putting It All Together 
    <div id="35-putting-it-all-together" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#35-putting-it-all-together" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Now that we’ve built all the key components of a Vision Transformer — patch embedding, positional encoding, multi-head self-attention, Transformer blocks, and the [CLS] token — it’s time to assemble everything into a full model.</p>
<p>In the code below:</p>
<ul>
<li>We use our <code>PatchEmbed</code>, <code>ViTEmbed</code>, and <code>TransformerBlock</code> classes from earlier.</li>
<li>The <code>[CLS]</code> token is passed through all transformer layers and then <strong>normalized</strong>.</li>
<li>We add a <strong>classification head</strong>: a single <code>nn.Linear</code> layer that maps the <code>[CLS]</code> token embedding to class logits.</li>
<li>This architecture mirrors the original <strong>ViT-Base</strong> (12 layers, 12 heads, 768-dim embeddings), but it’s easy to scale.</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SimpleViT</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">        <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span>
</span></span><span class="line"><span class="cl">    <span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbed</span><span class="p">(</span><span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">in_chans</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">num_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">vit_embed</span> <span class="o">=</span> <span class="n">ViTEmbed</span><span class="p">(</span><span class="n">num_patches</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Stack transformer blocks</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">            <span class="n">TransformerBlock</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Final normalization before classification</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="c1"># Linear classification head (using the CLS token)</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>  <span class="c1"># [batch_size, channels, height, width]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>         <span class="c1"># -&gt; [B, N, D]</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vit_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>           <span class="c1"># add CLS token + pos embed</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>              <span class="c1"># transformer layers</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>                <span class="c1"># normalize CLS token</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>       <span class="c1"># classification using CLS token</span>
</span></span></code></pre></div><hr>

<h2 class="relative group">4. Training the ViT 
    <div id="4-training-the-vit" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#4-training-the-vit" aria-label="Anchor">#</a>
    </span>        
    
</h2>

<h3 class="relative group">4.1 Dataset: CIFAR-10 
    <div id="41-dataset-cifar-10" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#41-dataset-cifar-10" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>We trained our Vision Transformer (ViT) on <strong>CIFAR-10</strong>, a well-known benchmark dataset with 60,000 images across 10 classes (e.g., airplanes, cats, ships). Each image is only 32×32 pixels, which makes CIFAR-10:</p>
<ul>
<li><em>Lightweight</em> and quick to train on</li>
<li><em>Challenging enough</em> to evaluate small models</li>
<li><em>Easy to visualize</em>, which helps interpret what the model is learning</li>
</ul>

<h3 class="relative group">4.2 Model Setup: Adapting ViT for CIFAR-10 
    <div id="42-model-setup-adapting-vit-for-cifar-10" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#42-model-setup-adapting-vit-for-cifar-10" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>ViTs were originally designed for large-scale datasets like ImageNet, so we made several adjustments to make training feasible on CIFAR-10 with limited compute:</p>
<ul>
<li><strong>Input size</strong>: Resized to 32×32 to match CIFAR-10</li>
<li><strong>Patch size</strong>: 4×4 → yields 64 tokens per image</li>
<li><strong>Embedding dimension</strong>: 192 (smaller than ImageNet-scale ViTs)</li>
<li><strong>Depth</strong>: 6 transformer blocks</li>
<li><strong>Heads</strong>: 3 attention heads (192 ÷ 3 = 64 dim per head)</li>
<li><strong>Positional embeddings</strong>: Recomputed for 64+1 tokens</li>
<li><strong>Batch size</strong>: 80 — balances speed and memory on Colab</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Refactored SimpleViT config for CIFAR-10</span>
</span></span><span class="line"><span class="cl"><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleViT</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">img_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>         <span class="c1"># CIFAR-10 images are 32x32</span>
</span></span><span class="line"><span class="cl">    <span class="n">patch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>        <span class="c1"># 4x4 patches → 64 tokens</span>
</span></span><span class="line"><span class="cl">    <span class="n">in_chans</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">embed_dim</span><span class="o">=</span><span class="mi">192</span><span class="p">,</span>       <span class="c1"># Smaller embedding size</span>
</span></span><span class="line"><span class="cl">    <span class="n">depth</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>             <span class="c1"># Fewer transformer blocks</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_heads</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>         <span class="c1"># Divides evenly into 192</span>
</span></span><span class="line"><span class="cl">    <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span>       <span class="c1"># For CIFAR-10</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></div>
<h3 class="relative group">4.3 Training Setup 
    <div id="43-training-setup" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#43-training-setup" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>The model was trained using:</p>
<ul>
<li><strong>Optimizer</strong>: Adam (<code>lr = 3e-4</code>)</li>
<li><strong>Loss</strong>: CrossEntropyLoss</li>
<li><strong>Hardware</strong>: Google Colab T4 GPU</li>
</ul>
<p>Training was efficient — about <strong>30 seconds per epoch</strong>, thanks to:</p>
<ul>
<li>Fewer transformer blocks and tokens</li>
<li>Larger batch size (80)</li>
<li>Explicit use of CUDA (<code>to(device)</code>)</li>
</ul>

<h3 class="relative group">4.3 Results 
    <div id="43-results" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#43-results" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>We trained our Vision Transformer for <strong>30 epochs</strong>, totaling ~15 minutes on a GPU. By the end of training, the model achieved approximately <strong>60% accuracy</strong> on the CIFAR-10 test set — a solid baseline given the model’s simplicity and the relatively small dataset size.</p>
<p>As shown in the training plots below:</p>
<ul>
<li><strong>Training loss</strong> steadily decreased, indicating that the model was effectively minimizing prediction error on the training set.</li>
<li><strong>Test accuracy</strong> improved rapidly within the first 10 epochs, plateauing around 60% thereafter. This suggests the model learned quickly but struggled to generalize further without additional techniques like data augmentation or regularization.</li>
</ul>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/ViTlossAccuracy.png" alt="" />
  <figcaption><strong>Figure 6:</strong> Training loss and test accuracy over 30 epochs. Source: author</figcaption>
  
  </figure>
<p>Here are a few <strong>example outputs</strong> from the model. While it correctly identified many samples (like cats and frogs), it struggled with visually similar classes (e.g., misclassifying a ship as an airplane).</p>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/ViTpred.png" alt="" />
  <figcaption><strong>Figure 7:</strong> Example predictions on CIFAR-10 images. Source: author</figcaption>
  
  </figure>
<p>The bar chart below shows how well the model performed <strong>across all 10 classes</strong>. Notably:</p>
<ul>
<li>The model performed best on <em>ship</em>, <em>automobile</em>, and <em>frog</em> classes — likely due to more distinctive visual features.</li>
<li>Performance lagged on <em>cat</em> and <em>bird</em>, which may be harder to distinguish due to higher intra-class variation and similar textures or shapes shared with other animals.</li>
</ul>

<figure>
      <img class="my-0 rounded-md" src="/img/vit/ViTaccPerClass.png" alt="" />
  <figcaption><strong>Figure 8:</strong> Accuracy by class. Source: author</figcaption>
  
  </figure>
<hr>

<h2 class="relative group">5. Limitations and Extensions 
    <div id="5-limitations-and-extensions" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#5-limitations-and-extensions" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Despite their success, Vision Transformers (ViTs) come with trade-offs. Here’s a summary of what to keep in mind:</p>

<h3 class="relative group">5.1 Limitations 
    <div id="51-limitations" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#51-limitations" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li>
<p><strong>Data-Hungry by Design</strong><br>
ViTs lack the strong inductive biases of CNNs (like locality and translation invariance), which means they <em>typically require large datasets</em> to perform well.<br>
→ <em>This is why the original ViT was pretrained on massive private datasets.</em></p>
</li>
<li>
<p><strong>Quadratic Time Complexity</strong><br>
The self-attention mechanism scales with the square of the number of input tokens — making ViTs <em>computationally expensive</em> for high-resolution images. For an image split into <code>N</code> patches, attention scales as <strong>O(N²)</strong>.</p>
</li>
</ul>

<h3 class="relative group">5.2 Extensions and Improvements 
    <div id="52-extensions-and-improvements" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#52-extensions-and-improvements" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Researchers have developed several workarounds and improvements to address these issues:</p>
<ul>
<li>
<p><strong>DeiT (Data-efficient Image Transformer)</strong><br>
A version of ViT trained <em>without large private datasets</em>, using <em>knowledge distillation</em> from a CNN teacher to improve performance on smaller datasets like ImageNet.</p>
</li>
<li>
<p><strong>Pretrained Backbones + Fine-Tuning</strong><br>
Instead of training ViTs from scratch, most modern pipelines use <em>pretrained ViTs</em> and then fine-tune them on downstream tasks with fewer samples.</p>
</li>
<li>
<p><strong>Swin Transformer</strong><br>
Introduces a <em>hierarchical</em> structure similar to CNNs by using local window-based attention that shifts across layers — making it <em>efficient</em> and <em>scalable</em> for high-resolution inputs.</p>
</li>
<li>
<p><strong>Fine-tuning on Small Datasets</strong><br>
Techniques like freezing early layers, adding task-specific heads, or leveraging self-supervised pretraining (e.g., DINO, MAE) can help ViTs adapt well to limited data.</p>
</li>
</ul>
<p>In short, while ViTs opened the door to attention-based vision modeling, their full potential is best realized when paired with large-scale pretraining, architectural tweaks, or smart training tricks.</p>
<hr>

<h2 class="relative group">6. GitHub + Colab 
    <div id="6-github--colab" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#6-github--colab" aria-label="Anchor">#</a>
    </span>        
    
</h2>

<h3 class="relative group">View the <a
  href="https://github.com/vickiiimu/vit-cifar10-tutorial"
    target="_blank"
  ><strong>Github</strong></a>
 
    <div id="view-the-github" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#view-the-github" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Includes a clean folder structure with:</p>
<ul>
<li><code>vit_cifar10.ipynb</code> notebook</li>
<li><code>images/</code> folder for visualizations</li>
<li><code>requirements.txt</code> for easy installation</li>
</ul>

<h3 class="relative group">Open in <a
  href="https://colab.research.google.com/drive/1vn57VeLHweWZCiEiJoc39q_NYUKuSXrc?usp=sharing"
    target="_blank"
  ><strong>Colab</strong></a>
 
    <div id="open-in-colab" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#open-in-colab" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>Readers can fork and run the notebook directly in the browser.</p>

<h3 class="relative group">Installation 
    <div id="installation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#installation" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">git clone https://github.com/vickiiimu/vit-cifar10-tutorial.git
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> vit-cifar10-tutorial
</span></span><span class="line"><span class="cl">pip install -r requirements.txt
</span></span></code></pre></div>
<h2 class="relative group">7. Conclusion 
    <div id="7-conclusion" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#7-conclusion" aria-label="Anchor">#</a>
    </span>        
    
</h2>
<p>Congratulations — you’ve just built a Vision Transformer from scratch!</p>
<p>Along the way, we covered the <strong>intuitions behind attention</strong>, walked through <strong>ViT’s architecture block-by-block</strong>, and reimplemented <strong>core components</strong> like patch embedding, positional encoding, and multi-head self-attention. If you followed the walkthrough, you now have a functioning ViT model you fully understand.</p>
<p>Whether you’re here to <em>learn the internals</em>, <em>prototype your own vision ideas</em>, or <em>just scratch the itch of curiosity</em> — this is your sandbox.</p>
<hr>

<h3 class="relative group">Feedback welcome 
    <div id="feedback-welcome" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#feedback-welcome" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>If you spot bugs, have suggestions, or build something cool on top of this, feel free to open an <a
  href="https://github.com/vickiiimu/vit-cifar10-tutorial/issues"
    target="_blank"
  >issue</a>
 or <a
  href="https://github.com/vickiiimu/vit-cifar10-tutorial/pulls"
    target="_blank"
  >pull request</a>
 on GitHub.</p>

<h3 class="relative group">Further reading 
    <div id="further-reading" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#further-reading" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<ul>
<li><a
  href="https://arxiv.org/abs/2010.11929"
    target="_blank"
  >An Image is Worth 16x16 Words (ViT paper)</a>
</li>
<li><a
  href="https://arxiv.org/abs/1706.03762"
    target="_blank"
  >Attention is All You Need (Transformer paper)</a>
</li>
<li><a
  href="https://arxiv.org/abs/2012.12877"
    target="_blank"
  >DeiT: Data-efficient training of ViTs</a>
</li>
</ul>
<p>Thanks for reading! 👋<br>
See you in the next post.</p>
<hr>

<h3 class="relative group">About Me 
    <div id="about-me" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#about-me" aria-label="Anchor">#</a>
    </span>        
    
</h3>
<p>I’m a sophomore at MIT studying physics and artificial intelligence. This   post is part of a series — <strong>Recreated from Scratch</strong> — where I reimplement   foundational AI papers to learn them inside and out. I’ll be posting new   walkthroughs every few weeks, diving into a different model or paper each time.</p>
<p>If you enjoyed this walkthrough, feel free to <a
  href="ttps://github.com/vickiiimu">follow me on GitHub</a>
, <a
  href="https://medium.com/@vicki.y.mu"
    target="_blank"
  >follow me on Medium</a>
, or even <a
  href="ttps://github.com/vickiiimu">reach out</a>
. I love chatting about research, open-source projects, and all things deep learning.</p>

          
          
          
        </div>

        

        
  <details class="mt-2 mb-5 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 margin-left-[0px]">
    
  <summary
    class="py-1 text-lg font-semibold cursor-pointer bg-primary-200 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-primary-800 dark:text-neutral-100">
    Recreated from Scratch -
    This article is part of a series.
  </summary>
  
  
    
      <div
        class="py-1 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600">
        Part 1:
        This Article
      </div>
    
  


  </details>


        
  
  <section class="flex flex-row flex-wrap justify-center pt-4 text-xl">
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://vickiiimu.github.io/my-site/posts/vit/&amp;title=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Share on LinkedIn"
          aria-label="Share on LinkedIn">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>

  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://twitter.com/intent/tweet/?url=https://vickiiimu.github.io/my-site/posts/vit/&amp;text=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Tweet on Twitter"
          aria-label="Tweet on Twitter">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://bsky.app/intent/compose?text=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images&#43;https://vickiiimu.github.io/my-site/posts/vit/"
          title="Post on Bluesky"
          aria-label="Post on Bluesky">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256,232.562c-21.183,-41.196 -78.868,-117.97 -132.503,-155.834c-51.378,-36.272 -70.978,-29.987 -83.828,-24.181c-14.872,6.72 -17.577,29.554 -17.577,42.988c0,13.433 7.365,110.138 12.169,126.281c15.873,53.336 72.376,71.358 124.413,65.574c2.66,-0.395 5.357,-0.759 8.089,-1.097c-2.68,0.429 -5.379,0.796 -8.089,1.097c-76.259,11.294 -143.984,39.085 -55.158,137.972c97.708,101.165 133.908,-21.692 152.484,-83.983c18.576,62.291 39.972,180.718 150.734,83.983c83.174,-83.983 22.851,-126.674 -53.408,-137.969c-2.71,-0.302 -5.409,-0.667 -8.089,-1.096c2.732,0.337 5.429,0.702 8.089,1.096c52.037,5.785 108.54,-12.239 124.413,-65.574c4.804,-16.142 12.169,-112.847 12.169,-126.281c-0,-13.434 -2.705,-36.267 -17.577,-42.988c-12.85,-5.806 -32.45,-12.09 -83.829,24.181c-53.634,37.864 -111.319,114.635 -132.502,155.831Z"/></svg>
  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://reddit.com/submit/?url=https://vickiiimu.github.io/my-site/posts/vit/&amp;resubmit=true&amp;title=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Submit to Reddit"
          aria-label="Submit to Reddit">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M201.5 305.5c-13.8 0-24.9-11.1-24.9-24.6 0-13.8 11.1-24.9 24.9-24.9 13.6 0 24.6 11.1 24.6 24.9 0 13.6-11.1 24.6-24.6 24.6zM504 256c0 137-111 248-248 248S8 393 8 256 119 8 256 8s248 111 248 248zm-132.3-41.2c-9.4 0-17.7 3.9-23.8 10-22.4-15.5-52.6-25.5-86.1-26.6l17.4-78.3 55.4 12.5c0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.3 24.9-24.9s-11.1-24.9-24.9-24.9c-9.7 0-18 5.8-22.1 13.8l-61.2-13.6c-3-.8-6.1 1.4-6.9 4.4l-19.1 86.4c-33.2 1.4-63.1 11.3-85.5 26.8-6.1-6.4-14.7-10.2-24.1-10.2-34.9 0-46.3 46.9-14.4 62.8-1.1 5-1.7 10.2-1.7 15.5 0 52.6 59.2 95.2 132 95.2 73.1 0 132.3-42.6 132.3-95.2 0-5.3-.6-10.8-1.9-15.8 31.3-16 19.8-62.5-14.9-62.5zM302.8 331c-18.2 18.2-76.1 17.9-93.6 0-2.2-2.2-6.1-2.2-8.3 0-2.5 2.5-2.5 6.4 0 8.6 22.8 22.8 87.3 22.8 110.2 0 2.5-2.2 2.5-6.1 0-8.6-2.2-2.2-6.1-2.2-8.3 0zm7.7-75c-13.6 0-24.6 11.1-24.6 24.9 0 13.6 11.1 24.6 24.6 24.6 13.8 0 24.9-11.1 24.9-24.6 0-13.8-11-24.9-24.9-24.9z"/></svg>

  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://api.whatsapp.com/send?text=https://vickiiimu.github.io/my-site/posts/vit/&amp;resubmit=true&amp;title=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Share via WhatsApp"
          aria-label="Share via WhatsApp">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M380.9 97.1C339 55.1 283.2 32 223.9 32c-122.4 0-222 99.6-222 222 0 39.1 10.2 77.3 29.6 111L0 480l117.7-30.9c32.4 17.7 68.9 27 106.1 27h.1c122.3 0 224.1-99.6 224.1-222 0-59.3-25.2-115-67.1-157zm-157 341.6c-33.2 0-65.7-8.9-94-25.7l-6.7-4-69.8 18.3L72 359.2l-4.4-7c-18.5-29.4-28.2-63.3-28.2-98.2 0-101.7 82.8-184.5 184.6-184.5 49.3 0 95.6 19.2 130.4 54.1 34.8 34.9 56.2 81.2 56.1 130.5 0 101.8-84.9 184.6-186.6 184.6zm101.2-138.2c-5.5-2.8-32.8-16.2-37.9-18-5.1-1.9-8.8-2.8-12.5 2.8-3.7 5.6-14.3 18-17.6 21.8-3.2 3.7-6.5 4.2-12 1.4-32.6-16.3-54-29.1-75.5-66-5.7-9.8 5.7-9.1 16.3-30.3 1.8-3.7.9-6.9-.5-9.7-1.4-2.8-12.5-30.1-17.1-41.2-4.5-10.8-9.1-9.3-12.5-9.5-3.2-.2-6.9-.2-10.6-.2-3.7 0-9.7 1.4-14.8 6.9-5.1 5.6-19.4 19-19.4 46.3 0 27.3 19.9 53.7 22.6 57.4 2.8 3.7 39.1 59.7 94.8 83.8 35.2 15.2 49 16.5 66.6 13.9 10.7-1.6 32.8-13.4 37.4-26.4 4.6-13 4.6-24.1 3.2-26.4-1.3-2.5-5-3.9-10.5-6.6z"/></svg>

  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://t.me/share/url?url=https://vickiiimu.github.io/my-site/posts/vit/&amp;resubmit=true&amp;title=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Share via Telegram"
          aria-label="Share via Telegram">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M248,8C111.033,8,0,119.033,0,256S111.033,504,248,504,496,392.967,496,256,384.967,8,248,8ZM362.952,176.66c-3.732,39.215-19.881,134.378-28.1,178.3-3.476,18.584-10.322,24.816-16.948,25.425-14.4,1.326-25.338-9.517-39.287-18.661-21.827-14.308-34.158-23.215-55.346-37.177-24.485-16.135-8.612-25,5.342-39.5,3.652-3.793,67.107-61.51,68.335-66.746.153-.655.3-3.1-1.154-4.384s-3.59-.849-5.135-.5q-3.283.746-104.608,69.142-14.845,10.194-26.894,9.934c-8.855-.191-25.888-5.006-38.551-9.123-15.531-5.048-27.875-7.717-26.8-16.291q.84-6.7,18.45-13.7,108.446-47.248,144.628-62.3c68.872-28.647,83.183-33.623,92.511-33.789,2.052-.034,6.639.474,9.61,2.885a10.452,10.452,0,0,1,3.53,6.716A43.765,43.765,0,0,1,362.952,176.66Z"/></svg>

  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://pinterest.com/pin/create/bookmarklet/?url=https://vickiiimu.github.io/my-site/posts/vit/&amp;description=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Pin on Pinterest"
          aria-label="Pin on Pinterest">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M496 256c0 137-111 248-248 248-25.6 0-50.2-3.9-73.4-11.1 10.1-16.5 25.2-43.5 30.8-65 3-11.6 15.4-59 15.4-59 8.1 15.4 31.7 28.5 56.8 28.5 74.8 0 128.7-68.8 128.7-154.3 0-81.9-66.9-143.2-152.9-143.2-107 0-163.9 71.8-163.9 150.1 0 36.4 19.4 81.7 50.3 96.1 4.7 2.2 7.2 1.2 8.3-3.3.8-3.4 5-20.3 6.9-28.1.6-2.5.3-4.7-1.7-7.1-10.1-12.5-18.3-35.3-18.3-56.6 0-54.7 41.4-107.6 112-107.6 60.9 0 103.6 41.5 103.6 100.9 0 67.1-33.9 113.6-78 113.6-24.3 0-42.6-20.1-36.7-44.8 7-29.5 20.5-61.3 20.5-82.6 0-19-10.2-34.9-31.4-34.9-24.9 0-44.9 25.7-44.9 60.2 0 22 7.4 36.8 7.4 36.8s-24.5 103.8-29 123.2c-5 21.4-3 51.6-.9 71.2C65.4 450.9 0 361.1 0 256 0 119 111 8 248 8s248 111 248 248z"/></svg>

  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://www.facebook.com/sharer/sharer.php?u=https://vickiiimu.github.io/my-site/posts/vit/&amp;quote=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Share on Facebook"
          aria-label="Share on Facebook">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M504 256C504 119 393 8 256 8S8 119 8 256c0 123.78 90.69 226.38 209.25 245V327.69h-63V256h63v-54.64c0-62.15 37-96.48 93.67-96.48 27.14 0 55.52 4.84 55.52 4.84v61h-31.28c-30.8 0-40.41 19.12-40.41 38.73V256h68.78l-11 71.69h-57.78V501C413.31 482.38 504 379.78 504 256z"/></svg>

  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="mailto:?body=https://vickiiimu.github.io/my-site/posts/vit/&amp;subject=From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Send via email"
          aria-label="Send via email">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1c-27.64 140.9 68.65 266.2 199.1 285.1c19.01 2.888 36.17-12.26 36.17-31.49l.0001-.6631c0-15.74-11.44-28.88-26.84-31.24c-84.35-12.98-149.2-86.13-149.2-174.2c0-102.9 88.61-185.5 193.4-175.4c91.54 8.869 158.6 91.25 158.6 183.2l0 16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98 .0036c-7.299 0-13.2 4.992-15.12 11.68c-24.85-12.15-54.24-16.38-86.06-5.106c-38.75 13.73-68.12 48.91-73.72 89.64c-9.483 69.01 43.81 128 110.9 128c26.44 0 50.43-9.544 69.59-24.88c24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3C495.1 107.1 361.2-9.332 207.8 20.73zM239.1 304.3c-26.47 0-48-21.56-48-48.05s21.53-48.05 48-48.05s48 21.56 48 48.05S266.5 304.3 239.1 304.3z"/></svg>

  </span>


        </a>
      
    
      
        <a
          target="_blank"
          class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800"
          href="https://line.me/R/share?text=https://vickiiimu.github.io/my-site/posts/vit/%20From%20Pixels%20to%20Predictions:%20Building%20a%20Transformer%20for%20Images"
          title="Share on LINE"
          aria-label="Share on LINE">
          

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--!Font Awesome Pro 6.4.2 by @fontawesome - https://fontawesome.com License -https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. --><path fill="currentColor" d="M311 196.8v81.3c0 2.1-1.6 3.7-3.7 3.7h-13c-1.3 0-2.4-.7-3-1.5l-37.3-50.3v48.2c0 2.1-1.6 3.7-3.7 3.7h-13c-2.1 0-3.7-1.6-3.7-3.7V196.9c0-2.1 1.6-3.7 3.7-3.7h12.9c1.1 0 2.4 .6 3 1.6l37.3 50.3V196.9c0-2.1 1.6-3.7 3.7-3.7h13c2.1-.1 3.8 1.6 3.8 3.5zm-93.7-3.7h-13c-2.1 0-3.7 1.6-3.7 3.7v81.3c0 2.1 1.6 3.7 3.7 3.7h13c2.1 0 3.7-1.6 3.7-3.7V196.8c0-1.9-1.6-3.7-3.7-3.7zm-31.4 68.1H150.3V196.8c0-2.1-1.6-3.7-3.7-3.7h-13c-2.1 0-3.7 1.6-3.7 3.7v81.3c0 1 .3 1.8 1 2.5c.7 .6 1.5 1 2.5 1h52.2c2.1 0 3.7-1.6 3.7-3.7v-13c0-1.9-1.6-3.7-3.5-3.7zm193.7-68.1H327.3c-1.9 0-3.7 1.6-3.7 3.7v81.3c0 1.9 1.6 3.7 3.7 3.7h52.2c2.1 0 3.7-1.6 3.7-3.7V265c0-2.1-1.6-3.7-3.7-3.7H344V247.7h35.5c2.1 0 3.7-1.6 3.7-3.7V230.9c0-2.1-1.6-3.7-3.7-3.7H344V213.5h35.5c2.1 0 3.7-1.6 3.7-3.7v-13c-.1-1.9-1.7-3.7-3.7-3.7zM512 93.4V419.4c-.1 51.2-42.1 92.7-93.4 92.6H92.6C41.4 511.9-.1 469.8 0 418.6V92.6C.1 41.4 42.2-.1 93.4 0H419.4c51.2 .1 92.7 42.1 92.6 93.4zM441.6 233.5c0-83.4-83.7-151.3-186.4-151.3s-186.4 67.9-186.4 151.3c0 74.7 66.3 137.4 155.9 149.3c21.8 4.7 19.3 12.7 14.4 42.1c-.8 4.7-3.8 18.4 16.1 10.1s107.3-63.2 146.5-108.2c27-29.7 39.9-59.8 39.9-93.1z" /></svg>
  </span>


        </a>
      
    
  </section>


        
  
  


      </div>

      
      
        
        
          
          
        
        
        
        <script
          type="text/javascript"
          src="/my-site/js/page.min.407e5b2727c1f241c95d53db24c776bea71bfc18e09511b815d669ad8caca6e9e18a53864ad364b1ccccfa2f2956768d33cfe193bfb64d3406f3b5ece354ba57.js"
          integrity="sha512-QH5bJyfB8kHJXVPbJMd2vqcb/BjglRG4FdZprYyspunhilOGStNksczM&#43;i8pVnaNM8/hk7&#43;2TTQG87Xs41S6Vw=="
          data-oid="views_posts/vit/index.md"
          data-oid-likes="likes_posts/vit/index.md"></script>
      

    </section>
    <footer class="pt-8 max-w-prose print:hidden">
      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600">
      <div class="flex justify-between pt-3">
        <span>
          
        </span>
        <span>
          
            <a class="flex text-right group ml-3" href="/my-site/posts/kaggle_imc/">
              <span class="flex flex-col">
                <span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Beginning on the Leaderboard: My First Kaggle Challenge in 7 Days</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                </span>
              </span>
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

        <div id="top-scroller" class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0">
  <a
    href="#the-top"
    class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="Scroll to top"
    title="Scroll to top">
    &uarr;
  </a>
</div>

      </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
      <nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400">
        <ul class="flex flex-col list-none sm:flex-row">
          
            <li
              class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href="/my-site/tags/"
                title="Tags">
                
                Tags
              </a>
            </li>
          
            <li
              class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0">
              <a
                class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center"
                href="/my-site/authors/"
                title="Authors">
                
                Authors
              </a>
            </li>
          
        </ul>
      </nav>
    
  
  <div class="flex items-center justify-between">
    
    
      <p class="text-sm text-neutral-500 dark:text-neutral-400">
          &copy;
          2025
          Vicki Mu
      </p>
    

    
    
      <p class="text-xs text-neutral-500 dark:text-neutral-400">
        
        
        Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
          href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a>
      </p>
    

  </div>
  <script>
    
    mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
      margin: 24,
      background: 'rgba(0,0,0,0.5)',
      scrollOffset: 0,
    })
    
  </script> 
  
  <script
    type="text/javascript"
    src="/my-site/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js"
    integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh] z-index-500"
  data-url="https://vickiiimu.github.io/my-site/">
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800">
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="Search"
          tabindex="0">
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="Close (Esc)">
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

    </div>
  </body>
  
</html>
